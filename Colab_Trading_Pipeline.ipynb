{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaheshUmale/COLAB_FILES/blob/main/Colab_Trading_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import zipfile\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "# --- COLAB SETUP & DATA DOWNLOAD ---\n",
        "# Data URL provided by the user\n",
        "DATA_URL = \"https://storage.googleapis.com/kaggle-data-sets/2575525/12691112/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20251001%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251001T235419Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0f9c7fd81c9801a691e9bed7879114c4c122987a0ed10445403f0dae54d50d333adaa1d0bf0af42b95facb7860a6d3d61a866409b2574ba9223c3c5ae97aea974508e75e9414c6ab9f021e4e2d7e521481da9595f4a2b30c0d6b6dd48a8e07d357386ae3594335aa2255f2c7862398e7d11086c46cca0e9698bdcbca9ae6b0a583f076699989cfbaef2e5486873e97a01f029f8545f502354ce75139f01362260f54831e820c7e67846d4800d69092b482c9a8992f1b509c99d4774c8e419c8c97b61b25f0ae591881ee45cd4e2b55779ba8ad98d7fd2a9cf3619926937851939e0dad53\"\n",
        "\n",
        "def setup_colab_environment(url, zip_name=\"archive.zip\", data_dir=\"/content/NSE_STOCK_DATA\"):\n",
        "    \"\"\"Downloads data, unzips it, and creates the target directory structure.\"\"\"\n",
        "\n",
        "    # Use shell command to download the file directly in Colab\n",
        "    print(f\"Downloading data from URL...\")\n",
        "    os.system(f\"wget -O {zip_name} \\\"{url}\\\"\")\n",
        "\n",
        "    # Create the directory to extract data into\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "\n",
        "    # Unzip the file\n",
        "    print(f\"Unzipping data into {data_dir}...\")\n",
        "    try:\n",
        "        # Note: The ZIP structure might contain a subfolder,\n",
        "        # so we extract all to the current directory and move them.\n",
        "        with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_dir)\n",
        "        print(\"Download and extraction complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during unzipping: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Search for files in the extracted directory to confirm path\n",
        "    # Colab path: /content/NSE_STOCK_DATA\n",
        "    # If the zip extracted to a subdirectory (e.g., /content/NSE_STOCK_DATA/bundle/archive)\n",
        "    # we need to adjust the path to the actual location of the CSV files.\n",
        "\n",
        "    # Simple check for the existence of files like '*_minute.csv'\n",
        "    if not any(f.endswith('_minute.csv') for f in os.listdir(data_dir)):\n",
        "        # If no files found, try traversing one level deeper (common Kaggle/Google Storage behavior)\n",
        "        print(\"CSV files not found directly in the target folder. Searching deeper...\")\n",
        "        for root, dirs, files in os.walk(data_dir):\n",
        "            if any(f.endswith('_minute.csv') for f in files):\n",
        "                data_dir = root\n",
        "                print(f\"Found CSV files in new directory: {data_dir}\")\n",
        "                break\n",
        "\n",
        "    return data_dir\n",
        "\n",
        "# --- START: DATA GENERATION LOGIC (Combined from generate_dataset_optimized.py) ---\n",
        "\n",
        "def calculate_rsi(df, window=14):\n",
        "    \"\"\"Calculates the Relative Strength Index (RSI).\"\"\"\n",
        "    delta = df['close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "    RS = np.where(loss == 0, np.inf, gain / loss)\n",
        "    df['rsi'] = 100 - (100 / (1 + RS))\n",
        "    return df\n",
        "\n",
        "def calculate_adx(df, window=14):\n",
        "    \"\"\"Calculates the Average Directional Index (ADX).\"\"\"\n",
        "    df['high_low'] = df['high'] - df['low']\n",
        "    df['high_prev_close'] = abs(df['high'] - df['close'].shift(1))\n",
        "    df['low_prev_close'] = abs(df['low'] - df['close'].shift(1))\n",
        "    df['tr'] = df[['high_low', 'high_prev_close', 'low_prev_close']].max(axis=1)\n",
        "\n",
        "    df['plus_dm'] = np.where((df['high'] > df['high'].shift(1)) & (df['high'] - df['high'].shift(1) > df['low'].shift(1) - df['low']), df['high'] - df['high'].shift(1), 0)\n",
        "    df['minus_dm'] = np.where((df['low'].shift(1) > df['low']) & (df['low'].shift(1) - df['low'] > df['high'] - df['high'].shift(1)), df['low'].shift(1) - df['low'], 0)\n",
        "\n",
        "    temp_atr = df['tr'].ewm(alpha=1/window, adjust=False).mean()\n",
        "    df['plus_di'] = (df['plus_dm'].ewm(alpha=1/window, adjust=False).mean() / temp_atr) * 100\n",
        "    df['minus_di'] = (df['minus_dm'].ewm(alpha=1/window, adjust=False).mean() / temp_atr) * 100\n",
        "\n",
        "    df['dx'] = abs(df['plus_di'] - df['minus_di']) / (df['plus_di'] + df['minus_di']) * 100\n",
        "    df['adx'] = df['dx'].ewm(alpha=1/window, adjust=False).mean()\n",
        "\n",
        "    df.drop(columns=['high_low', 'high_prev_close', 'low_prev_close', 'tr', 'plus_dm', 'minus_dm', 'plus_di', 'minus_di', 'dx'], inplace=True, errors='ignore')\n",
        "    return df\n",
        "\n",
        "def calculate_vwap(df):\n",
        "    \"\"\"Calculates the Volume Weighted Average Price (VWAP) for the entire dataset.\"\"\"\n",
        "    df['typical_price'] = (df['high'] + df['low'] + df['close']) / 3\n",
        "    df['tpv'] = df['typical_price'] * df['volume']\n",
        "\n",
        "    # Calculate VWAP cumulatively (VWAP usually resets daily, but simplified here)\n",
        "    df['cum_tpv'] = df['tpv'].cumsum()\n",
        "    df['cum_volume'] = df['volume'].cumsum()\n",
        "    df['vwap'] = df['cum_tpv'] / df['cum_volume']\n",
        "\n",
        "    df.drop(columns=['typical_price', 'tpv', 'cum_tpv', 'cum_volume'], inplace=True, errors='ignore')\n",
        "    return df\n",
        "\n",
        "def calculate_indicators(df, atr_period=14, bb_period=20, kc_period=20, kc_multiplier=2.0):\n",
        "    \"\"\"Calculates TTM Squeeze, RVOL, RSI, ADX, SMA distances, and VWAP.\"\"\"\n",
        "\n",
        "    # ATR Calculation (Must be first for KC and Normalization)\n",
        "    df['high_low'] = df['high'] - df['low']\n",
        "    df['high_prev_close'] = abs(df['high'] - df['close'].shift(1))\n",
        "    df['low_prev_close'] = abs(df['low'] - df['close'].shift(1))\n",
        "    df['tr'] = df[['high_low', 'high_prev_close', 'low_prev_close']].max(axis=1)\n",
        "    df['atr'] = df['tr'].ewm(alpha=1/atr_period, adjust=False).mean()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    df['bb_sma'] = df['close'].rolling(window=bb_period).mean()\n",
        "    df['bb_std'] = df['close'].rolling(window=bb_period).std()\n",
        "    df['bb_upper'] = df['bb_sma'] + (df['bb_std'] * 2)\n",
        "    df['bb_lower'] = df['bb_sma'] - (df['bb_std'] * 2)\n",
        "\n",
        "    # Keltner Channels\n",
        "    df['kc_sma'] = df['close'].rolling(window=kc_period).mean()\n",
        "    df['kc_upper'] = df['kc_sma'] + (df['atr'] * kc_multiplier)\n",
        "    df['kc_lower'] = df['kc_sma'] - (df['atr'] * kc_multiplier)\n",
        "\n",
        "    # RVOL\n",
        "    df['avg_volume'] = df['volume'].rolling(window=20).mean()\n",
        "    df['rvol'] = df['volume'] / df['avg_volume']\n",
        "\n",
        "    # Squeeze Status\n",
        "    df['squeeze_on'] = (df['bb_lower'] > df['kc_lower']) & (df['bb_upper'] < df['kc_upper'])\n",
        "\n",
        "    # NEW: Other Indicators\n",
        "    df = calculate_rsi(df)\n",
        "    df = calculate_adx(df)\n",
        "    df = calculate_vwap(df)\n",
        "\n",
        "    # SMA Distance Calculation\n",
        "    df['sma_50'] = df['close'].rolling(window=50).mean()\n",
        "    df['sma_200'] = df['close'].rolling(window=200).mean()\n",
        "\n",
        "    df['dist_from_sma_50'] = (df['close'] - df['sma_50']) / df['atr']\n",
        "    df['dist_from_sma_200'] = (df['close'] - df['sma_200']) / df['atr']\n",
        "\n",
        "    # Drop intermediate columns\n",
        "    df.drop(['high_low', 'high_prev_close', 'low_prev_close', 'tr',\n",
        "             'avg_volume', 'bb_sma', 'bb_std', 'kc_sma', 'atr',\n",
        "             'sma_50', 'sma_200'],\n",
        "            axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_and_resample_data(directory, symbol, timeframe_minutes):\n",
        "    \"\"\"Loads and resamples 1-minute data, returning None on error.\"\"\"\n",
        "    filename = f\"{symbol}_minute.csv\"\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    if not os.path.exists(filepath):\n",
        "        # Colab's default unzip behavior might put files directly in /content\n",
        "        alt_filepath = os.path.join('/content', filename)\n",
        "        if os.path.exists(alt_filepath):\n",
        "             filepath = alt_filepath\n",
        "        else:\n",
        "            # print(f\"Data file not found: {filepath}\")\n",
        "            return None\n",
        "\n",
        "    # Use low_memory=False for better performance on larger CSVs\n",
        "    df = pd.read_csv(filepath, index_col='date', parse_dates=True, low_memory=False)\n",
        "\n",
        "    if timeframe_minutes == 1440:\n",
        "        resample_period = '1D'\n",
        "    else:\n",
        "        resample_period = f'{timeframe_minutes}min'\n",
        "\n",
        "    resampled_df = df.resample(resample_period).agg({\n",
        "        'open': 'first',\n",
        "        'high': 'max',\n",
        "        'low': 'min',\n",
        "        'close': 'last',\n",
        "        'volume': 'sum'\n",
        "    }).dropna()\n",
        "\n",
        "    return resampled_df\n",
        "\n",
        "def generate_training_data(primary_df, mtf_dfs, symbol, primary_timeframe_min):\n",
        "    \"\"\"\n",
        "    Generates training data for the primary timeframe (3min), looking up MTF context.\n",
        "    \"\"\"\n",
        "    training_data = []\n",
        "\n",
        "    # Identify MTF contexts (Timeframes other than the primary one)\n",
        "    mtf_timeframes = sorted([tf for tf in mtf_dfs.keys() if tf != primary_timeframe_min])\n",
        "\n",
        "    for i in range(200, len(primary_df) - 1): # Start after 200 bars to ensure all indicators are populated\n",
        "        signal = False\n",
        "        direction = None\n",
        "\n",
        "        was_in_squeeze = primary_df['squeeze_on'].iloc[i-1]\n",
        "        is_in_squeeze = primary_df['squeeze_on'].iloc[i]\n",
        "\n",
        "        current_close = primary_df['close'].iloc[i]\n",
        "        current_vwap = primary_df['vwap'].iloc[i]\n",
        "\n",
        "        # 1. TTM Squeeze Breakout Check\n",
        "        is_bullish_breakout = was_in_squeeze and not is_in_squeeze and current_close > primary_df['bb_upper'].iloc[i]\n",
        "        is_bearish_breakout = was_in_squeeze and not is_in_squeeze and current_close < primary_df['bb_lower'].iloc[i]\n",
        "\n",
        "        # 2. RVOL Filter\n",
        "        rvol_ok = primary_df['rvol'].iloc[i] > 2\n",
        "\n",
        "        # 3. VWAP Filter\n",
        "        vwap_ok = False\n",
        "        if is_bullish_breakout:\n",
        "            vwap_ok = current_close > current_vwap\n",
        "            if vwap_ok:\n",
        "                signal = True\n",
        "                direction = 'long'\n",
        "        elif is_bearish_breakout:\n",
        "            vwap_ok = current_close < current_vwap\n",
        "            if vwap_ok:\n",
        "                signal = True\n",
        "                direction = 'short'\n",
        "\n",
        "        # Final Signal Check\n",
        "        if signal and rvol_ok:\n",
        "\n",
        "            # Capture base features\n",
        "            features = {\n",
        "                'symbol': symbol,\n",
        "                'timeframe': primary_timeframe_min,\n",
        "                'date': primary_df.index[i],\n",
        "                'direction': direction,\n",
        "                'rvol': primary_df['rvol'].iloc[i],\n",
        "                'bb_width': primary_df['bb_upper'].iloc[i] - primary_df['bb_lower'].iloc[i],\n",
        "                'kc_width': primary_df['kc_upper'].iloc[i] - primary_df['kc_lower'].iloc[i],\n",
        "                'close_bb_dist': primary_df['close'].iloc[i] - primary_df['bb_upper'].iloc[i] if direction == 'long' else primary_df['close'].iloc[i] - primary_df['bb_lower'].iloc[i],\n",
        "                'rsi': primary_df['rsi'].iloc[i],\n",
        "                'adx': primary_df['adx'].iloc[i],\n",
        "                'dist_from_sma_50': primary_df['dist_from_sma_50'].iloc[i],\n",
        "                'dist_from_sma_200': primary_df['dist_from_sma_200'].iloc[i],\n",
        "                'vwap_diff': primary_df['vwap'].iloc[i] - primary_df['close'].iloc[i]\n",
        "            }\n",
        "\n",
        "            # Capture MTF Confluence features\n",
        "            current_time = primary_df.index[i]\n",
        "            for tf_min in mtf_timeframes:\n",
        "                tf_label = f'is_{tf_min}min_sqz' if tf_min != 1440 else 'is_1D_sqz'\n",
        "\n",
        "                if tf_min == 1440:\n",
        "                    lookup_time = current_time.normalize() # Start of the day\n",
        "                else:\n",
        "                    lookup_time = current_time.floor(f'{tf_min}min')\n",
        "\n",
        "                # Look up the squeeze status in the higher timeframe DF\n",
        "                try:\n",
        "                    # Use index slicing for more robust lookup near floor time\n",
        "                    mtf_bar = mtf_dfs[tf_min].loc[mtf_dfs[tf_min].index <= current_time].iloc[-1]\n",
        "                    features[tf_label] = int(mtf_bar['squeeze_on'])\n",
        "                except (KeyError, IndexError):\n",
        "                    # If lookup fails or no earlier bar exists, treat as not in squeeze (0)\n",
        "                    features[tf_label] = 0\n",
        "\n",
        "            # Simulate the trade to get the outcome (label)\n",
        "            # Check for sufficient future data (i+1 for entry, minimum i+2 for exit bar)\n",
        "            if i + 1 >= len(primary_df):\n",
        "                 continue # Skip if no entry bar is available\n",
        "\n",
        "            entry_price = primary_df['open'].iloc[i+1]\n",
        "            label = 0  # Default to loss\n",
        "\n",
        "            # Risk/Reward (using the same 1:2 logic)\n",
        "            if direction == 'long':\n",
        "                risk = entry_price - primary_df['bb_lower'].iloc[i]\n",
        "                stop_loss = entry_price - risk\n",
        "                take_profit = entry_price + (2 * risk)\n",
        "            else: # short\n",
        "                risk = primary_df['bb_upper'].iloc[i] - entry_price\n",
        "                stop_loss = entry_price + risk\n",
        "                take_profit = entry_price - (2 * risk)\n",
        "\n",
        "            # Look ahead to find the exit\n",
        "            for j in range(i + 1, len(primary_df)):\n",
        "                current_low = primary_df['low'].iloc[j]\n",
        "                current_high = primary_df['high'].iloc[j]\n",
        "                exit_found = False\n",
        "\n",
        "                if direction == 'long':\n",
        "                    if current_high >= take_profit:\n",
        "                        label = 1\n",
        "                        exit_found = True\n",
        "                    elif current_low <= stop_loss:\n",
        "                        label = 0\n",
        "                        exit_found = True\n",
        "                else: # short\n",
        "                    if current_low <= take_profit:\n",
        "                        label = 1\n",
        "                        exit_found = True\n",
        "                    elif current_high >= stop_loss:\n",
        "                        label = 0\n",
        "                        exit_found = True\n",
        "\n",
        "                if exit_found:\n",
        "                    features['outcome'] = label\n",
        "                    training_data.append(features)\n",
        "                    break\n",
        "\n",
        "    return pd.DataFrame(training_data)\n",
        "\n",
        "def process_single_symbol(symbol, directory, timeframes_to_test):\n",
        "    \"\"\"\n",
        "    Loads ALL timeframes for a symbol, processes them, and generates training data\n",
        "    from the PRIMARY timeframe (lowest TF) using the others for context.\n",
        "    \"\"\"\n",
        "    primary_timeframe_min = min(timeframes_to_test)\n",
        "    mtf_dfs = {}\n",
        "\n",
        "    # print(f\"--- START: Loading and calculating data for {symbol} ---\")\n",
        "\n",
        "    # 1. Load and Calculate Indicators for ALL Timeframes\n",
        "    for timeframe in timeframes_to_test:\n",
        "        df = load_and_resample_data(directory, symbol, timeframe)\n",
        "        if df is not None:\n",
        "            try:\n",
        "                df = calculate_indicators(df)\n",
        "                df.dropna(inplace=True)\n",
        "                mtf_dfs[timeframe] = df\n",
        "                # print(f\"[{symbol}] Calculated features for {timeframe}min.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating indicators for {symbol} ({timeframe}min): {e}\")\n",
        "\n",
        "    # 2. Generate Training Data (Only from the Primary TF)\n",
        "    # Require at least one bar of each timeframe plus 200 bars for indicator lookback\n",
        "    if primary_timeframe_min in mtf_dfs and len(mtf_dfs[primary_timeframe_min]) > 200:\n",
        "        primary_df = mtf_dfs[primary_timeframe_min]\n",
        "        try:\n",
        "            symbol_dataset = generate_training_data(primary_df, mtf_dfs, symbol, primary_timeframe_min)\n",
        "\n",
        "            if not symbol_dataset.empty:\n",
        "                return symbol_dataset\n",
        "            else:\n",
        "                 pass # print(f\"[{symbol}] Primary TF ({primary_timeframe_min}min) generated no data after filters.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error generating training data for {symbol}: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "def load_symbols_from_dir(directory):\n",
        "    \"\"\"Extracts stock symbols from filenames in a directory.\"\"\"\n",
        "    csv_filenames = os.listdir(directory)\n",
        "    pattern = r\"(.*)_minute.csv\"\n",
        "    symbols = []\n",
        "    for filename in csv_filenames:\n",
        "        match = re.search(pattern, filename)\n",
        "        if match:\n",
        "            symbols.append(match.group(1))\n",
        "    return symbols\n",
        "\n",
        "# --- END: DATA GENERATION LOGIC ---\n",
        "\n",
        "# --- START: MODEL TRAINING LOGIC (Combined from model_training.py) ---\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import joblib\n",
        "\n",
        "def preprocess_and_split(data):\n",
        "    \"\"\"Handles time-series splitting, feature encoding, and scaling.\"\"\"\n",
        "    # 1. Define Features and Label\n",
        "    X_features = [\n",
        "        'rvol', 'bb_width', 'kc_width', 'close_bb_dist', 'rsi', 'adx',\n",
        "        'dist_from_sma_50', 'dist_from_sma_200', 'vwap_diff',\n",
        "        'is_5min_sqz', 'is_15min_sqz', 'is_30min_sqz', 'is_60min_sqz', 'is_1D_sqz'\n",
        "    ]\n",
        "\n",
        "    categorical_features = ['direction']\n",
        "    all_features = X_features + categorical_features\n",
        "\n",
        "    X = data[all_features]\n",
        "    y = data['outcome']\n",
        "\n",
        "    # 2. Time-Series Split (80% Train, 20% Test)\n",
        "    test_size = 0.2\n",
        "    split_index = int(len(X) * (1 - test_size))\n",
        "\n",
        "    X_train_raw = X.iloc[:split_index]\n",
        "    X_test_raw = X.iloc[split_index:]\n",
        "    y_train = y.iloc[:split_index]\n",
        "    y_test = y.iloc[split_index:]\n",
        "\n",
        "    # 3. Preprocessing Pipeline\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), X_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    return X_train_raw, X_test_raw, y_train, y_test, preprocessor\n",
        "\n",
        "def train_and_evaluate(X_train_raw, X_test_raw, y_train, y_test, preprocessor):\n",
        "    \"\"\"Trains a Random Forest Classifier and evaluates its performance.\"\"\"\n",
        "\n",
        "    # Scikit-learn Random Forest Classifier optimized for CPU cores (n_jobs=-1)\n",
        "    model = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(\n",
        "            n_estimators=500,\n",
        "            max_depth=15,\n",
        "            random_state=42,\n",
        "            class_weight='balanced',\n",
        "            n_jobs=-1               # Use all CPU cores!\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    print(\"\\n--- Starting Model Training (Leveraging all Colab CPU cores) ---\")\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train_raw, y_train)\n",
        "    end_time = time.time()\n",
        "    print(f\"Training complete in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = model.predict(X_test_raw)\n",
        "    y_proba = model.predict_proba(X_test_raw)[:, 1]\n",
        "\n",
        "    print(\"\\n--- Model Evaluation on Test Data ---\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Loss (0)', 'Win (1)']))\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    print(\"\\nConfusion Matrix (Rows=Actual, Cols=Predicted):\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    try:\n",
        "        auc_score = roc_auc_score(y_test, y_proba)\n",
        "        print(f\"AUC Score: {auc_score:.4f}\")\n",
        "    except ValueError:\n",
        "        print(\"AUC Score requires at least one sample of each class in the test set.\")\n",
        "\n",
        "    # Calculate Precision for the Win class (index 1)\n",
        "    tp = conf_matrix[1, 1]\n",
        "    fp = conf_matrix[0, 1]\n",
        "    win_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    print(f\"\\nPrecision (Win Class): {win_precision:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    model_filepath = 'squeeze_classifier_model.pkl'\n",
        "    joblib.dump(model, model_filepath)\n",
        "    print(f\"\\nModel saved successfully as '{model_filepath}' in your Colab environment.\")\n",
        "\n",
        "    return model, win_precision\n",
        "\n",
        "# --- END: MODEL TRAINING LOGIC ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # 1. SETUP & DOWNLOAD DATA\n",
        "    # ----------------------------------------------------------------\n",
        "\n",
        "    # This function handles the shell command for download and unzip\n",
        "    DATA_DIRECTORY = setup_colab_environment(DATA_URL)\n",
        "\n",
        "    if DATA_DIRECTORY is None:\n",
        "        print(\"\\nFATAL ERROR: Could not set up data directory. Cannot proceed.\")\n",
        "    else:\n",
        "        # ----------------------------------------------------------------\n",
        "        # 2. DATA GENERATION PIPELINE\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "        timeframes_to_test = [3, 5, 15, 30, 60, 1440] # 1440 = 1 Day\n",
        "        output_path = \"training_dataset.csv\"\n",
        "        primary_tf = min(timeframes_to_test)\n",
        "\n",
        "        # Setup Output File and Header\n",
        "        if os.path.exists(output_path):\n",
        "             os.remove(output_path) # Delete old file to ensure clean run\n",
        "             print(f\"Existing file '{output_path}' deleted for fresh generation.\")\n",
        "\n",
        "        header_df = pd.DataFrame(columns=[\n",
        "            'symbol', 'timeframe', 'date', 'direction', 'rvol',\n",
        "            'bb_width', 'kc_width', 'close_bb_dist', 'rsi', 'adx',\n",
        "            'dist_from_sma_50', 'dist_from_sma_200', 'vwap_diff',\n",
        "            'is_5min_sqz', 'is_15min_sqz', 'is_30min_sqz', 'is_60min_sqz', 'is_1D_sqz',\n",
        "            'outcome'\n",
        "        ])\n",
        "        header_df.to_csv(output_path, index=False)\n",
        "        print(f\"\\nStarting data generation. Signals generated only on {primary_tf}min with MTF context.\")\n",
        "\n",
        "        all_symbols = load_symbols_from_dir(DATA_DIRECTORY)\n",
        "        if not all_symbols:\n",
        "            print(\"No stock data files found in the specified directory.\")\n",
        "\n",
        "        total_data_points = 0\n",
        "\n",
        "        # Parallel Processing\n",
        "        start_gen_time = time.time()\n",
        "        with ProcessPoolExecutor(max_workers=None) as executor:\n",
        "\n",
        "            future_to_symbol = {\n",
        "                executor.submit(process_single_symbol, symbol, DATA_DIRECTORY, timeframes_to_test): symbol\n",
        "                for symbol in all_symbols\n",
        "            }\n",
        "\n",
        "            for future in as_completed(future_to_symbol):\n",
        "                symbol = future_to_symbol[future]\n",
        "                try:\n",
        "                    symbol_result_df = future.result()\n",
        "\n",
        "                    if symbol_result_df is not None and not symbol_result_df.empty:\n",
        "                        symbol_result_df.to_csv(output_path, mode='a', header=False, index=False)\n",
        "                        total_data_points += len(symbol_result_df)\n",
        "                        print(f\"--- COMPLETE: {symbol} data ({len(symbol_result_df)} points) appended. Total: {total_data_points} ---\")\n",
        "                    # else: print(f\"--- COMPLETE: {symbol} generated no training data. ---\")\n",
        "\n",
        "                except Exception as exc:\n",
        "                    print(f\"!!! Major error processing symbol {symbol}: {exc}\")\n",
        "\n",
        "        end_gen_time = time.time()\n",
        "        print(f\"\\n--- Data generation complete in {end_gen_time - start_gen_time:.2f} seconds. ---\")\n",
        "        print(f\"Total data points saved to {output_path}: {total_data_points}\")\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # 3. MODEL TRAINING PIPELINE\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "        if total_data_points > 100:\n",
        "            data = pd.read_csv(output_path)\n",
        "            data['direction'] = data['direction'].astype(str)\n",
        "\n",
        "            X_train_raw, X_test_raw, y_train, y_test, preprocessor = preprocess_and_split(data)\n",
        "\n",
        "            # Train and Evaluate\n",
        "            train_and_evaluate(X_train_raw, X_test_raw, y_train, y_test, preprocessor)\n",
        "        else:\n",
        "            print(\"\\nSkipping training: Insufficient data points generated.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from URL...\n",
            "Unzipping data into /content/NSE_STOCK_DATA...\n",
            "Error during unzipping: File is not a zip file\n",
            "\n",
            "FATAL ERROR: Could not set up data directory. Cannot proceed.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Oqr5J9I68sQ",
        "outputId": "de74ba94-5ea6-4319-901e-1ec15153a2c5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Completed\")"
      ],
      "metadata": {
        "id": "b3i5vgCd-PnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import zipfile\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "# --- COLAB SETUP & DATA DOWNLOAD ---\n",
        "# Data URL provided by the user\n",
        "# NOTE: YOU MUST REPLACE THIS WITH A FRESHLY GENERATED DOWNLOAD LINK FROM KAGGLE\n",
        "DATA_URL = \"https://storage.googleapis.com/kaggle-data-sets/2575525/12691112/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20251002%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251002T080620Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0c80f67373b4d541a02b9e9b2585185813fcbdffd2583841fc53206bdcea6f9b013f96313831630a1b93732f8664ec7ae92bb665ab25d00c707b4740a95d262e983ab4fc168fa23a5d62c27146b710c65c44385c486d194fb8385869ddc2a040d0a1269840c3996448f1be1ed8a735838fb94037b6862eb71438acb5e7a0a5844a57996c2c79bec966e025ab3093f2ce91f6a652b593c98f1dbb2ce86e46b9511cf5a7282f833fc8f3aed4c3b1224eb68599dbb7a8eee4319e2b488894eb1e7370338888d37987656571b9d62cab65e0118689d3e021ee7f74032b7466b7cdebba8a79e57d07f3cd341bb84367baf5a5680dc63c514627989c38140f1977a5eb\"\n",
        "\n",
        "def setup_colab_environment(url, zip_name=\"data_archive.zip\", data_dir=\"/content/NSE_STOCK_DATA\"):\n",
        "    \"\"\"Downloads data, unzips it, and creates the target directory structure.\"\"\"\n",
        "\n",
        "    # Use curl with -L (follow redirects) for better handling of signed URLs\n",
        "    print(f\"Downloading data from URL using curl...\")\n",
        "    # Execute the download command\n",
        "    download_command = f\"curl -L -o {zip_name} \\\"{url}\\\"\"\n",
        "    return_code = os.system(download_command)\n",
        "\n",
        "    if return_code != 0 or not os.path.exists(zip_name):\n",
        "        print(f\"\\nERROR: Download command failed (Curl exit code {return_code}). The signed URL may have expired.\")\n",
        "\n",
        "    # Create the directory to extract data into\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "\n",
        "    # Unzip the file\n",
        "    print(f\"\\nUnzipping data into {data_dir}...\")\n",
        "    try:\n",
        "        if not os.path.exists(zip_name) or not zipfile.is_zipfile(zip_name):\n",
        "             # This means the URL was likely expired and downloaded an HTML error page.\n",
        "             print(\"CRITICAL ERROR: File downloaded but is not a valid ZIP file. The download URL likely expired.\")\n",
        "             return None\n",
        "\n",
        "        with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_dir)\n",
        "        print(\"Download and extraction complete.\")\n",
        "        # Clean up the downloaded zip file after successful extraction\n",
        "        os.remove(zip_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during unzipping: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Search for files in the extracted directory to confirm path\n",
        "    # Colab path: /content/NSE_STOCK_DATA\n",
        "    # If the zip extracted to a subdirectory (e.g., /content/NSE_STOCK_DATA/bundle/archive)\n",
        "    # we need to adjust the path to the actual location of the CSV files.\n",
        "\n",
        "    # Simple check for the existence of files like '*_minute.csv'\n",
        "    if not any(f.endswith('_minute.csv') for f in os.listdir(data_dir)):\n",
        "        # If no files found, try traversing one level deeper (common Kaggle/Google Storage behavior)\n",
        "        print(\"CSV files not found directly in the target folder. Searching deeper...\")\n",
        "        for root, dirs, files in os.walk(data_dir):\n",
        "            if any(f.endswith('_minute.csv') for f in files):\n",
        "                data_dir = root\n",
        "                print(f\"Found CSV files in new directory: {data_dir}\")\n",
        "                break\n",
        "\n",
        "    return data_dir\n",
        "\n",
        "# --- START: DATA GENERATION LOGIC (Combined from generate_dataset_optimized.py) ---\n",
        "# ... (All other functions are unchanged)\n",
        "\n",
        "def calculate_rsi(df, window=14):\n",
        "    \"\"\"Calculates the Relative Strength Index (RSI).\"\"\"\n",
        "    delta = df['close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "    RS = np.where(loss == 0, np.inf, gain / loss)\n",
        "    df['rsi'] = 100 - (100 / (1 + RS))\n",
        "    return df\n",
        "\n",
        "def calculate_adx(df, window=14):\n",
        "    \"\"\"Calculates the Average Directional Index (ADX).\"\"\"\n",
        "    df['high_low'] = df['high'] - df['low']\n",
        "    df['high_prev_close'] = abs(df['high'] - df['close'].shift(1))\n",
        "    df['low_prev_close'] = abs(df['low'] - df['close'].shift(1))\n",
        "    df['tr'] = df[['high_low', 'high_prev_close', 'low_prev_close']].max(axis=1)\n",
        "\n",
        "    df['plus_dm'] = np.where((df['high'] > df['high'].shift(1)) & (df['high'] - df['high'].shift(1) > df['low'].shift(1) - df['low']), df['high'] - df['high'].shift(1), 0)\n",
        "    df['minus_dm'] = np.where((df['low'].shift(1) > df['low']) & (df['low'].shift(1) - df['low'] > df['high'] - df['high'].shift(1)), df['low'].shift(1) - df['low'], 0)\n",
        "\n",
        "    temp_atr = df['tr'].ewm(alpha=1/window, adjust=False).mean()\n",
        "    df['plus_di'] = (df['plus_dm'].ewm(alpha=1/window, adjust=False).mean() / temp_atr) * 100\n",
        "    df['minus_di'] = (df['minus_dm'].ewm(alpha=1/window, adjust=False).mean() / temp_atr) * 100\n",
        "\n",
        "    df['dx'] = abs(df['plus_di'] - df['minus_di']) / (df['plus_di'] + df['minus_di']) * 100\n",
        "    df['adx'] = df['dx'].ewm(alpha=1/window, adjust=False).mean()\n",
        "\n",
        "    df.drop(columns=['high_low', 'high_prev_close', 'low_prev_close', 'tr', 'plus_dm', 'minus_dm', 'plus_di', 'minus_di', 'dx'], inplace=True, errors='ignore')\n",
        "    return df\n",
        "\n",
        "def calculate_vwap(df):\n",
        "    \"\"\"Calculates the Volume Weighted Average Price (VWAP) for the entire dataset.\"\"\"\n",
        "    df['typical_price'] = (df['high'] + df['low'] + df['close']) / 3\n",
        "    df['tpv'] = df['typical_price'] * df['volume']\n",
        "\n",
        "    # Calculate VWAP cumulatively (VWAP usually resets daily, but simplified here)\n",
        "    df['cum_tpv'] = df['tpv'].cumsum()\n",
        "    df['cum_volume'] = df['volume'].cumsum()\n",
        "    df['vwap'] = df['cum_tpv'] / df['cum_volume']\n",
        "\n",
        "    df.drop(columns=['typical_price', 'tpv', 'cum_tpv', 'cum_volume'], inplace=True, errors='ignore')\n",
        "    return df\n",
        "\n",
        "def calculate_indicators(df, atr_period=14, bb_period=20, kc_period=20, kc_multiplier=2.0):\n",
        "    \"\"\"Calculates TTM Squeeze, RVOL, RSI, ADX, SMA distances, and VWAP.\"\"\"\n",
        "\n",
        "    # ATR Calculation (Must be first for KC and Normalization)\n",
        "    df['high_low'] = df['high'] - df['low']\n",
        "    df['high_prev_close'] = abs(df['high'] - df['close'].shift(1))\n",
        "    df['low_prev_close'] = abs(df['low'] - df['close'].shift(1))\n",
        "    df['tr'] = df[['high_low', 'high_prev_close', 'low_prev_close']].max(axis=1)\n",
        "    df['atr'] = df['tr'].ewm(alpha=1/atr_period, adjust=False).mean()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    df['bb_sma'] = df['close'].rolling(window=bb_period).mean()\n",
        "    df['bb_std'] = df['close'].rolling(window=bb_period).std()\n",
        "    df['bb_upper'] = df['bb_sma'] + (df['bb_std'] * 2)\n",
        "    df['bb_lower'] = df['bb_sma'] - (df['bb_std'] * 2)\n",
        "\n",
        "    # Keltner Channels\n",
        "    df['kc_sma'] = df['close'].rolling(window=kc_period).mean()\n",
        "    df['kc_upper'] = df['kc_sma'] + (df['atr'] * kc_multiplier)\n",
        "    df['kc_lower'] = df['kc_sma'] - (df['atr'] * kc_multiplier)\n",
        "\n",
        "    # RVOL\n",
        "    df['avg_volume'] = df['volume'].rolling(window=20).mean()\n",
        "    df['rvol'] = df['volume'] / df['avg_volume']\n",
        "\n",
        "    # Squeeze Status\n",
        "    df['squeeze_on'] = (df['bb_lower'] > df['kc_lower']) & (df['bb_upper'] < df['kc_upper'])\n",
        "\n",
        "    # NEW: Other Indicators\n",
        "    df = calculate_rsi(df)\n",
        "    df = calculate_adx(df)\n",
        "    df = calculate_vwap(df)\n",
        "\n",
        "    # SMA Distance Calculation\n",
        "    df['sma_50'] = df['close'].rolling(window=50).mean()\n",
        "    df['sma_200'] = df['close'].rolling(window=200).mean()\n",
        "\n",
        "    df['dist_from_sma_50'] = (df['close'] - df['sma_50']) / df['atr']\n",
        "    df['dist_from_sma_200'] = (df['close'] - df['sma_200']) / df['atr']\n",
        "\n",
        "    # Drop intermediate columns\n",
        "    df.drop(['high_low', 'high_prev_close', 'low_prev_close', 'tr',\n",
        "             'avg_volume', 'bb_sma', 'bb_std', 'kc_sma', 'atr',\n",
        "             'sma_50', 'sma_200'],\n",
        "            axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_and_resample_data(directory, symbol, timeframe_minutes):\n",
        "    \"\"\"Loads and resamples 1-minute data, returning None on error.\"\"\"\n",
        "    filename = f\"{symbol}_minute.csv\"\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    if not os.path.exists(filepath):\n",
        "        # Colab's default unzip behavior might put files directly in /content\n",
        "        alt_filepath = os.path.join('/content', filename)\n",
        "        if os.path.exists(alt_filepath):\n",
        "             filepath = alt_filepath\n",
        "        else:\n",
        "            # print(f\"Data file not found: {filepath}\")\n",
        "            return None\n",
        "\n",
        "    # Use low_memory=False for better performance on larger CSVs\n",
        "    df = pd.read_csv(filepath, index_col='date', parse_dates=True, low_memory=False)\n",
        "\n",
        "    if timeframe_minutes == 1440:\n",
        "        resample_period = '1D'\n",
        "    else:\n",
        "        resample_period = f'{timeframe_minutes}min'\n",
        "\n",
        "    resampled_df = df.resample(resample_period).agg({\n",
        "        'open': 'first',\n",
        "        'high': 'max',\n",
        "        'low': 'min',\n",
        "        'close': 'last',\n",
        "        'volume': 'sum'\n",
        "    }).dropna()\n",
        "\n",
        "    return resampled_df\n",
        "\n",
        "def generate_training_data(primary_df, mtf_dfs, symbol, primary_timeframe_min):\n",
        "    \"\"\"\n",
        "    Generates training data for the primary timeframe (3min), looking up MTF context.\n",
        "    \"\"\"\n",
        "    training_data = []\n",
        "\n",
        "    # Identify MTF contexts (Timeframes other than the primary one)\n",
        "    mtf_timeframes = sorted([tf for tf in mtf_dfs.keys() if tf != primary_timeframe_min])\n",
        "\n",
        "    for i in range(200, len(primary_df) - 1): # Start after 200 bars to ensure all indicators are populated\n",
        "        signal = False\n",
        "        direction = None\n",
        "\n",
        "        was_in_squeeze = primary_df['squeeze_on'].iloc[i-1]\n",
        "        is_in_squeeze = primary_df['squeeze_on'].iloc[i]\n",
        "\n",
        "        current_close = primary_df['close'].iloc[i]\n",
        "        current_vwap = primary_df['vwap'].iloc[i]\n",
        "\n",
        "        # 1. TTM Squeeze Breakout Check\n",
        "        is_bullish_breakout = was_in_squeeze and not is_in_squeeze and current_close > primary_df['bb_upper'].iloc[i]\n",
        "        is_bearish_breakout = was_in_squeeze and not is_in_squeeze and current_close < primary_df['bb_lower'].iloc[i]\n",
        "\n",
        "        # 2. RVOL Filter\n",
        "        rvol_ok = primary_df['rvol'].iloc[i] > 2\n",
        "\n",
        "        # 3. VWAP Filter\n",
        "        vwap_ok = False\n",
        "        if is_bullish_breakout:\n",
        "            vwap_ok = current_close > current_vwap\n",
        "            if vwap_ok:\n",
        "                signal = True\n",
        "                direction = 'long'\n",
        "        elif is_bearish_breakout:\n",
        "            vwap_ok = current_close < current_vwap\n",
        "            if vwap_ok:\n",
        "                signal = True\n",
        "                direction = 'short'\n",
        "\n",
        "        # Final Signal Check\n",
        "        if signal and rvol_ok:\n",
        "\n",
        "            # Capture base features\n",
        "            features = {\n",
        "                'symbol': symbol,\n",
        "                'timeframe': primary_timeframe_min,\n",
        "                'date': primary_df.index[i],\n",
        "                'direction': direction,\n",
        "                'rvol': primary_df['rvol'].iloc[i],\n",
        "                'bb_width': primary_df['bb_upper'].iloc[i] - primary_df['bb_lower'].iloc[i],\n",
        "                'kc_width': primary_df['kc_upper'].iloc[i] - primary_df['kc_lower'].iloc[i],\n",
        "                'close_bb_dist': primary_df['close'].iloc[i] - primary_df['bb_upper'].iloc[i] if direction == 'long' else primary_df['close'].iloc[i] - primary_df['bb_lower'].iloc[i],\n",
        "                'rsi': primary_df['rsi'].iloc[i],\n",
        "                'adx': primary_df['adx'].iloc[i],\n",
        "                'dist_from_sma_50': primary_df['dist_from_sma_50'].iloc[i],\n",
        "                'dist_from_sma_200': primary_df['dist_from_sma_200'].iloc[i],\n",
        "                'vwap_diff': primary_df['vwap'].iloc[i] - primary_df['close'].iloc[i]\n",
        "            }\n",
        "\n",
        "            # Capture MTF Confluence features\n",
        "            current_time = primary_df.index[i]\n",
        "            for tf_min in mtf_timeframes:\n",
        "                tf_label = f'is_{tf_min}min_sqz' if tf_min != 1440 else 'is_1D_sqz'\n",
        "\n",
        "                if tf_min == 1440:\n",
        "                    lookup_time = current_time.normalize() # Start of the day\n",
        "                else:\n",
        "                    lookup_time = current_time.floor(f'{tf_min}min')\n",
        "\n",
        "                # Look up the squeeze status in the higher timeframe DF\n",
        "                try:\n",
        "                    # Use index slicing for more robust lookup near floor time\n",
        "                    mtf_bar = mtf_dfs[tf_min].loc[mtf_dfs[tf_min].index <= current_time].iloc[-1]\n",
        "                    features[tf_label] = int(mtf_bar['squeeze_on'])\n",
        "                except (KeyError, IndexError):\n",
        "                    # If lookup fails or no earlier bar exists, treat as not in squeeze (0)\n",
        "                    features[tf_label] = 0\n",
        "\n",
        "            # Simulate the trade to get the outcome (label)\n",
        "            # Check for sufficient future data (i+1 for entry, minimum i+2 for exit bar)\n",
        "            if i + 1 >= len(primary_df):\n",
        "                 continue # Skip if no entry bar is available\n",
        "\n",
        "            entry_price = primary_df['open'].iloc[i+1]\n",
        "            label = 0  # Default to loss\n",
        "\n",
        "            # Risk/Reward (using the same 1:2 logic)\n",
        "            if direction == 'long':\n",
        "                risk = entry_price - primary_df['bb_lower'].iloc[i]\n",
        "                stop_loss = entry_price - risk\n",
        "                take_profit = entry_price + (2 * risk)\n",
        "            else: # short\n",
        "                risk = primary_df['bb_upper'].iloc[i] - entry_price\n",
        "                stop_loss = entry_price + risk\n",
        "                take_profit = entry_price - (2 * risk)\n",
        "\n",
        "            # Look ahead to find the exit\n",
        "            for j in range(i + 1, len(primary_df)):\n",
        "                current_low = primary_df['low'].iloc[j]\n",
        "                current_high = primary_df['high'].iloc[j]\n",
        "                exit_found = False\n",
        "\n",
        "                if direction == 'long':\n",
        "                    if current_high >= take_profit:\n",
        "                        label = 1\n",
        "                        exit_found = True\n",
        "                    elif current_low <= stop_loss:\n",
        "                        label = 0\n",
        "                        exit_found = True\n",
        "                else: # short\n",
        "                    if current_low <= take_profit:\n",
        "                        label = 1\n",
        "                        exit_found = True\n",
        "                    elif current_high >= stop_loss:\n",
        "                        label = 0\n",
        "                        exit_found = True\n",
        "\n",
        "                if exit_found:\n",
        "                    features['outcome'] = label\n",
        "                    training_data.append(features)\n",
        "                    break\n",
        "\n",
        "    return pd.DataFrame(training_data)\n",
        "\n",
        "def process_single_symbol(symbol, directory, timeframes_to_test):\n",
        "    \"\"\"\n",
        "    Loads ALL timeframes for a symbol, processes them, and generates training data\n",
        "    from the PRIMARY timeframe (lowest TF) using the others for context.\n",
        "    \"\"\"\n",
        "    primary_timeframe_min = min(timeframes_to_test)\n",
        "    mtf_dfs = {}\n",
        "\n",
        "    # print(f\"--- START: Loading and calculating data for {symbol} ---\")\n",
        "\n",
        "    # 1. Load and Calculate Indicators for ALL Timeframes\n",
        "    for timeframe in timeframes_to_test:\n",
        "        df = load_and_resample_data(directory, symbol, timeframe)\n",
        "        if df is not None:\n",
        "            try:\n",
        "                df = calculate_indicators(df)\n",
        "                df.dropna(inplace=True)\n",
        "                mtf_dfs[timeframe] = df\n",
        "                # print(f\"[{symbol}] Calculated features for {timeframe}min.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating indicators for {symbol} ({timeframe}min): {e}\")\n",
        "\n",
        "    # 2. Generate Training Data (Only from the Primary TF)\n",
        "    # Require at least one bar of each timeframe plus 200 bars for indicator lookback\n",
        "    if primary_timeframe_min in mtf_dfs and len(mtf_dfs[primary_timeframe_min]) > 200:\n",
        "        primary_df = mtf_dfs[primary_timeframe_min]\n",
        "        try:\n",
        "            symbol_dataset = generate_training_data(primary_df, mtf_dfs, symbol, primary_timeframe_min)\n",
        "\n",
        "            if not symbol_dataset.empty:\n",
        "                return symbol_dataset\n",
        "            else:\n",
        "                 pass # print(f\"[{symbol}] Primary TF ({primary_timeframe_min}min) generated no data after filters.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error generating training data for {symbol}: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "def load_symbols_from_dir(directory):\n",
        "    \"\"\"Extracts stock symbols from filenames in a directory.\"\"\"\n",
        "    csv_filenames = os.listdir(directory)\n",
        "    pattern = r\"(.*)_minute.csv\"\n",
        "    symbols = []\n",
        "    for filename in csv_filenames:\n",
        "        match = re.search(pattern, filename)\n",
        "        if match:\n",
        "            symbols.append(match.group(1))\n",
        "    return symbols\n",
        "\n",
        "# --- END: DATA GENERATION LOGIC ---\n",
        "\n",
        "# --- START: MODEL TRAINING LOGIC (Combined from model_training.py) ---\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import joblib\n",
        "\n",
        "def preprocess_and_split(data):\n",
        "    \"\"\"Handles time-series splitting, feature encoding, and scaling.\"\"\"\n",
        "    # 1. Define Features and Label\n",
        "    X_features = [\n",
        "        'rvol', 'bb_width', 'kc_width', 'close_bb_dist', 'rsi', 'adx',\n",
        "        'dist_from_sma_50', 'dist_from_sma_200', 'vwap_diff',\n",
        "        'is_5min_sqz', 'is_15min_sqz', 'is_30min_sqz', 'is_60min_sqz', 'is_1D_sqz'\n",
        "    ]\n",
        "\n",
        "    categorical_features = ['direction']\n",
        "    all_features = X_features + categorical_features\n",
        "\n",
        "    X = data[all_features]\n",
        "    y = data['outcome']\n",
        "\n",
        "    # 2. Time-Series Split (80% Train, 20% Test)\n",
        "    test_size = 0.2\n",
        "    split_index = int(len(X) * (1 - test_size))\n",
        "\n",
        "    X_train_raw = X.iloc[:split_index]\n",
        "    X_test_raw = X.iloc[split_index:]\n",
        "    y_train = y.iloc[:split_index]\n",
        "    y_test = y.iloc[split_index:]\n",
        "\n",
        "    # 3. Preprocessing Pipeline\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), X_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    return X_train_raw, X_test_raw, y_train, y_test, preprocessor\n",
        "\n",
        "def train_and_evaluate(X_train_raw, X_test_raw, y_train, y_test, preprocessor):\n",
        "    \"\"\"Trains a Random Forest Classifier and evaluates its performance.\"\"\"\n",
        "\n",
        "    # Scikit-learn Random Forest Classifier optimized for CPU cores (n_jobs=-1)\n",
        "    model = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(\n",
        "            n_estimators=500,\n",
        "            max_depth=15,\n",
        "            random_state=42,\n",
        "            class_weight='balanced',\n",
        "            n_jobs=-1               # Use all CPU cores!\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    print(\"\\n--- Starting Model Training (Leveraging all Colab CPU cores) ---\")\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train_raw, y_train)\n",
        "    end_time = time.time()\n",
        "    print(f\"Training complete in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = model.predict(X_test_raw)\n",
        "    y_proba = model.predict_proba(X_test_raw)[:, 1]\n",
        "\n",
        "    print(\"\\n--- Model Evaluation on Test Data ---\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Loss (0)', 'Win (1)']))\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    print(\"\\nConfusion Matrix (Rows=Actual, Cols=Predicted):\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    try:\n",
        "        auc_score = roc_auc_score(y_test, y_proba)\n",
        "        print(f\"AUC Score: {auc_score:.4f}\")\n",
        "    except ValueError:\n",
        "        print(\"AUC Score requires at least one sample of each class in the test set.\")\n",
        "\n",
        "    # Calculate Precision for the Win class (index 1)\n",
        "    tp = conf_matrix[1, 1]\n",
        "    fp = conf_matrix[0, 1]\n",
        "    win_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    print(f\"\\nPrecision (Win Class): {win_precision:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    model_filepath = 'squeeze_classifier_model.pkl'\n",
        "    joblib.dump(model, model_filepath)\n",
        "    print(f\"\\nModel saved successfully as '{model_filepath}' in your Colab environment.\")\n",
        "\n",
        "    return model, win_precision\n",
        "\n",
        "# --- END: MODEL TRAINING LOGIC ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # 1. SETUP & DOWNLOAD DATA\n",
        "    # ----------------------------------------------------------------\n",
        "\n",
        "    # This function handles the shell command for download and unzip\n",
        "    DATA_DIRECTORY = setup_colab_environment(DATA_URL)\n",
        "\n",
        "    if DATA_DIRECTORY is None:\n",
        "        print(\"\\nFATAL ERROR: Could not set up data directory. Cannot proceed.\")\n",
        "    else:\n",
        "        # ----------------------------------------------------------------\n",
        "        # 2. DATA GENERATION PIPELINE\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "        timeframes_to_test = [3, 5, 15, 30, 60, 1440] # 1440 = 1 Day\n",
        "        output_path = \"training_dataset.csv\"\n",
        "        primary_tf = min(timeframes_to_test)\n",
        "\n",
        "        # Setup Output File and Header\n",
        "        if os.path.exists(output_path):\n",
        "             os.remove(output_path) # Delete old file to ensure clean run\n",
        "             print(f\"Existing file '{output_path}' deleted for fresh generation.\")\n",
        "\n",
        "        header_df = pd.DataFrame(columns=[\n",
        "            'symbol', 'timeframe', 'date', 'direction', 'rvol',\n",
        "            'bb_width', 'kc_width', 'close_bb_dist', 'rsi', 'adx',\n",
        "            'dist_from_sma_50', 'dist_from_sma_200', 'vwap_diff',\n",
        "            'is_5min_sqz', 'is_15min_sqz', 'is_30min_sqz', 'is_60min_sqz', 'is_1D_sqz',\n",
        "            'outcome'\n",
        "        ])\n",
        "        header_df.to_csv(output_path, index=False)\n",
        "        print(f\"\\nStarting data generation. Signals generated only on {primary_tf}min with MTF context.\")\n",
        "\n",
        "        all_symbols = load_symbols_from_dir(DATA_DIRECTORY)\n",
        "        if not all_symbols:\n",
        "            print(\"No stock data files found in the specified directory.\")\n",
        "\n",
        "        total_data_points = 0\n",
        "\n",
        "        # Parallel Processing\n",
        "        start_gen_time = time.time()\n",
        "        with ProcessPoolExecutor(max_workers=None) as executor:\n",
        "\n",
        "            future_to_symbol = {\n",
        "                executor.submit(process_single_symbol, symbol, DATA_DIRECTORY, timeframes_to_test): symbol\n",
        "                for symbol in all_symbols\n",
        "            }\n",
        "\n",
        "            for future in as_completed(future_to_symbol):\n",
        "                symbol = future_to_symbol[future]\n",
        "                try:\n",
        "                    symbol_result_df = future.result()\n",
        "\n",
        "                    if symbol_result_df is not None and not symbol_result_df.empty:\n",
        "                        symbol_result_df.to_csv(output_path, mode='a', header=False, index=False)\n",
        "                        total_data_points += len(symbol_result_df)\n",
        "                        print(f\"--- COMPLETE: {symbol} data ({len(symbol_result_df)} points) appended. Total: {total_data_points} ---\")\n",
        "                    # else: print(f\"--- COMPLETE: {symbol} generated no training data. ---\")\n",
        "\n",
        "                except Exception as exc:\n",
        "                    print(f\"!!! Major error processing symbol {symbol}: {exc}\")\n",
        "\n",
        "        end_gen_time = time.time()\n",
        "        print(f\"\\n--- Data generation complete in {end_gen_time - start_gen_time:.2f} seconds. ---\")\n",
        "        print(f\"Total data points saved to {output_path}: {total_data_points}\")\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # 3. MODEL TRAINING PIPELINE\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "        if total_data_points > 100:\n",
        "            data = pd.read_csv(output_path)\n",
        "            data['direction'] = data['direction'].astype(str)\n",
        "\n",
        "            X_train_raw, X_test_raw, y_train, y_test, preprocessor = preprocess_and_split(data)\n",
        "\n",
        "            # Train and Evaluate\n",
        "            train_and_evaluate(X_train_raw, X_test_raw, y_train, y_test, preprocessor)\n",
        "        else:\n",
        "            print(\"\\nSkipping training: Insufficient data points generated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVxiNeor9f26",
        "outputId": "1ad7581b-66a5-40a5-8f9d-a7f5c0731300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from URL using curl...\n",
            "\n",
            "Unzipping data into /content/NSE_STOCK_DATA...\n",
            "Download and extraction complete.\n",
            "\n",
            "Starting data generation. Signals generated only on 3min with MTF context.\n",
            "--- COMPLETE: PAYTM data (492 points) appended. Total: 492 ---\n",
            "--- COMPLETE: STARHEALTH data (408 points) appended. Total: 900 ---\n",
            "--- COMPLETE: NEWGEN data (713 points) appended. Total: 1613 ---\n",
            "--- COMPLETE: GRAPHITE data (1208 points) appended. Total: 2821 ---\n",
            "--- COMPLETE: GRANULES data (1267 points) appended. Total: 4088 ---\n",
            "--- COMPLETE: PFIZER data (958 points) appended. Total: 5046 ---\n",
            "--- COMPLETE: DOMS data (175 points) appended. Total: 5221 ---\n",
            "--- COMPLETE: ALKEM data (869 points) appended. Total: 6090 ---\n",
            "--- COMPLETE: BHARTIARTL data (873 points) appended. Total: 6963 ---\n",
            "--- COMPLETE: CIEINDIA data (1024 points) appended. Total: 7987 ---\n",
            "--- COMPLETE: SUPREMEIND data (865 points) appended. Total: 8852 ---\n",
            "--- COMPLETE: BALRAMCHIN data (1333 points) appended. Total: 10185 ---\n",
            "--- COMPLETE: MMFIN data (997 points) appended. Total: 11182 ---\n",
            "--- COMPLETE: CGPOWER data (1083 points) appended. Total: 12265 ---\n",
            "--- COMPLETE: RAYMOND data (1360 points) appended. Total: 13625 ---\n",
            "--- COMPLETE: VARROC data (724 points) appended. Total: 14349 ---\n",
            "--- COMPLETE: DIVISLAB data (1031 points) appended. Total: 15380 ---\n",
            "--- COMPLETE: ADANIENT data (1359 points) appended. Total: 16739 ---\n",
            "--- COMPLETE: LICHSGFIN data (1017 points) appended. Total: 17756 ---\n",
            "--- COMPLETE: GODREJIND data (1094 points) appended. Total: 18850 ---\n",
            "--- COMPLETE: NAM-INDIA data (736 points) appended. Total: 19586 ---\n",
            "--- COMPLETE: ADANIENSOL data (1125 points) appended. Total: 20711 ---\n",
            "--- COMPLETE: WESTLIFE data (574 points) appended. Total: 21285 ---\n",
            "--- COMPLETE: KAJARIACER data (922 points) appended. Total: 22207 ---\n",
            "--- COMPLETE: BHARTIHEXA data (139 points) appended. Total: 22346 ---\n",
            "--- COMPLETE: LTIM data (850 points) appended. Total: 23196 ---\n",
            "--- COMPLETE: CENTURYPLY data (1028 points) appended. Total: 24224 ---\n",
            "--- COMPLETE: GPPL data (937 points) appended. Total: 25161 ---\n",
            "--- COMPLETE: POLYCAB data (700 points) appended. Total: 25861 ---\n",
            "--- COMPLETE: ONGC data (955 points) appended. Total: 26816 ---\n",
            "--- COMPLETE: CERA data (738 points) appended. Total: 27554 ---\n",
            "--- COMPLETE: LODHA data (492 points) appended. Total: 28046 ---\n",
            "--- COMPLETE: GLAND data (385 points) appended. Total: 28431 ---\n",
            "--- COMPLETE: TRIDENT data (1000 points) appended. Total: 29431 ---\n",
            "--- COMPLETE: WIPRO data (961 points) appended. Total: 30392 ---\n",
            "--- COMPLETE: GRINFRA data (350 points) appended. Total: 30742 ---\n",
            "--- COMPLETE: SBFC data (165 points) appended. Total: 30907 ---\n",
            "--- COMPLETE: APARINDS data (938 points) appended. Total: 31845 ---\n",
            "--- COMPLETE: DEVYANI data (424 points) appended. Total: 32269 ---\n",
            "--- COMPLETE: MM data (861 points) appended. Total: 33130 ---\n",
            "--- COMPLETE: POLICYBZR data (351 points) appended. Total: 33481 ---\n",
            "--- COMPLETE: NHPC data (668 points) appended. Total: 34149 ---\n",
            "--- COMPLETE: BATAINDIA data (1072 points) appended. Total: 35221 ---\n",
            "--- COMPLETE: ICICIPRULI data (784 points) appended. Total: 36005 ---\n",
            "--- COMPLETE: HFCL data (1171 points) appended. Total: 37176 ---\n",
            "--- COMPLETE: LLOYDSME data (237 points) appended. Total: 37413 ---\n",
            "--- COMPLETE: AREM data (1078 points) appended. Total: 38491 ---\n",
            "--- COMPLETE: COROMANDEL data (850 points) appended. Total: 39341 ---\n",
            "--- COMPLETE: SAREGAMA data (920 points) appended. Total: 40261 ---\n",
            "--- COMPLETE: BIRLACORPN data (897 points) appended. Total: 41158 ---\n",
            "--- COMPLETE: LTTS data (896 points) appended. Total: 42054 ---\n",
            "--- COMPLETE: PNCINFRA data (932 points) appended. Total: 42986 ---\n",
            "--- COMPLETE: PEL data (1171 points) appended. Total: 44157 ---\n",
            "--- COMPLETE: MANKIND data (215 points) appended. Total: 44372 ---\n",
            "--- COMPLETE: FACT data (1136 points) appended. Total: 45508 ---\n",
            "--- COMPLETE: MUTHOOTFIN data (1039 points) appended. Total: 46547 ---\n",
            "--- COMPLETE: SANOFI data (744 points) appended. Total: 47291 ---\n",
            "--- COMPLETE: DIXON data (936 points) appended. Total: 48227 ---\n",
            "--- COMPLETE: SUMICHEM data (592 points) appended. Total: 48819 ---\n",
            "--- COMPLETE: UNITDSPR data (1098 points) appended. Total: 49917 ---\n",
            "--- COMPLETE: PRAJIND data (1212 points) appended. Total: 51129 ---\n",
            "--- COMPLETE: CAPLIPOINT data (1062 points) appended. Total: 52191 ---\n",
            "--- COMPLETE: DALBHARAT data (552 points) appended. Total: 52743 ---\n",
            "--- COMPLETE: GSFC data (1178 points) appended. Total: 53921 ---\n",
            "--- COMPLETE: UPL data (1066 points) appended. Total: 54987 ---\n",
            "--- COMPLETE: RELIANCE data (1084 points) appended. Total: 56071 ---\n",
            "--- COMPLETE: MCX data (1274 points) appended. Total: 57345 ---\n",
            "--- COMPLETE: BPCL data (1074 points) appended. Total: 58419 ---\n",
            "--- COMPLETE: UTIAMC data (486 points) appended. Total: 58905 ---\n",
            "--- COMPLETE: 3MINDIA data (701 points) appended. Total: 59606 ---\n",
            "--- COMPLETE: JUBLFOOD data (1229 points) appended. Total: 60835 ---\n",
            "--- COMPLETE: NLCINDIA data (1059 points) appended. Total: 61894 ---\n",
            "--- COMPLETE: RCF data (1195 points) appended. Total: 63089 ---\n",
            "--- COMPLETE: PETRONET data (870 points) appended. Total: 63959 ---\n",
            "--- COMPLETE: MRF data (1203 points) appended. Total: 65162 ---\n",
            "--- COMPLETE: ZYDUSLIFE data (950 points) appended. Total: 66112 ---\n",
            "--- COMPLETE: SAMMAANCAP data (1074 points) appended. Total: 67186 ---\n",
            "--- COMPLETE: PIIND data (903 points) appended. Total: 68089 ---\n",
            "--- COMPLETE: BAJAJFINSV data (1197 points) appended. Total: 69286 ---\n",
            "--- COMPLETE: SONATSOFTW data (1135 points) appended. Total: 70421 ---\n",
            "--- COMPLETE: MOTILALOFS data (1000 points) appended. Total: 71421 ---\n",
            "--- COMPLETE: BHEL data (1129 points) appended. Total: 72550 ---\n",
            "--- COMPLETE: COFORGE data (1298 points) appended. Total: 73848 ---\n",
            "--- COMPLETE: BALAMINES data (1034 points) appended. Total: 74882 ---\n",
            "--- COMPLETE: BEL data (1061 points) appended. Total: 75943 ---\n",
            "--- COMPLETE: JINDALSTEL data (1319 points) appended. Total: 77262 ---\n",
            "--- COMPLETE: BAJFINANCE data (1213 points) appended. Total: 78475 ---\n",
            "--- COMPLETE: SAIL data (1173 points) appended. Total: 79648 ---\n",
            "--- COMPLETE: CREDITACC data (724 points) appended. Total: 80372 ---\n",
            "--- COMPLETE: WELCORP data (1248 points) appended. Total: 81620 ---\n",
            "--- COMPLETE: TITAN data (1097 points) appended. Total: 82717 ---\n",
            "--- COMPLETE: HOMEFIRST data (476 points) appended. Total: 83193 ---\n",
            "--- COMPLETE: SUNTV data (1182 points) appended. Total: 84375 ---\n",
            "--- COMPLETE: KPRMILL data (906 points) appended. Total: 85281 ---\n",
            "--- COMPLETE: FINPIPE data (988 points) appended. Total: 86269 ---\n",
            "--- COMPLETE: JYOTHYLAB data (1023 points) appended. Total: 87292 ---\n",
            "--- COMPLETE: FLUOROCHEM data (600 points) appended. Total: 87892 ---\n",
            "--- COMPLETE: AXISBANK data (956 points) appended. Total: 88848 ---\n",
            "--- COMPLETE: CRISIL data (922 points) appended. Total: 89770 ---\n",
            "--- COMPLETE: INOXINDIA data (184 points) appended. Total: 89954 ---\n",
            "--- COMPLETE: CASTROLIND data (889 points) appended. Total: 90843 ---\n",
            "--- COMPLETE: FINCABLES data (1051 points) appended. Total: 91894 ---\n",
            "--- COMPLETE: IRCTC data (1076 points) appended. Total: 92970 ---\n",
            "--- COMPLETE: JKLAKSHMI data (968 points) appended. Total: 93938 ---\n",
            "--- COMPLETE: GODFRYPHLP data (1276 points) appended. Total: 95214 ---\n",
            "--- COMPLETE: SWSOLAR data (671 points) appended. Total: 95885 ---\n",
            "--- COMPLETE: BHARATFORG data (1044 points) appended. Total: 96929 ---\n",
            "--- COMPLETE: 360ONE data (538 points) appended. Total: 97467 ---\n",
            "--- COMPLETE: SUNDARMFIN data (846 points) appended. Total: 98313 ---\n",
            "--- COMPLETE: PAGEIND data (1019 points) appended. Total: 99332 ---\n",
            "--- COMPLETE: EQUITASBNK data (424 points) appended. Total: 99756 ---\n",
            "--- COMPLETE: MAXHEALTH data (507 points) appended. Total: 100263 ---\n",
            "--- COMPLETE: AJANTPHARM data (1129 points) appended. Total: 101392 ---\n",
            "--- COMPLETE: POWERINDIA data (562 points) appended. Total: 101954 ---\n",
            "--- COMPLETE: VEDL data (1283 points) appended. Total: 103237 ---\n",
            "--- COMPLETE: HDFCAMC data (701 points) appended. Total: 103938 ---\n",
            "--- COMPLETE: TANLA data (1112 points) appended. Total: 105050 ---\n",
            "--- COMPLETE: KNRCON data (908 points) appended. Total: 105958 ---\n",
            "--- COMPLETE: CHALET data (646 points) appended. Total: 106604 ---\n",
            "--- COMPLETE: MOTHERSON data (997 points) appended. Total: 107601 ---\n",
            "--- COMPLETE: HINDPETRO data (1019 points) appended. Total: 108620 ---\n",
            "--- COMPLETE: ASTRAL data (843 points) appended. Total: 109463 ---\n",
            "--- COMPLETE: MSUMI data (236 points) appended. Total: 109699 ---\n",
            "--- COMPLETE: CHOLAFIN data (945 points) appended. Total: 110644 ---\n",
            "--- COMPLETE: EIDPARRY data (1098 points) appended. Total: 111742 ---\n",
            "--- COMPLETE: MANYAVAR data (274 points) appended. Total: 112016 ---\n",
            "--- COMPLETE: HUDCO data (731 points) appended. Total: 112747 ---\n",
            "--- COMPLETE: RBLBANK data (936 points) appended. Total: 113683 ---\n",
            "--- COMPLETE: LT data (888 points) appended. Total: 114571 ---\n",
            "--- COMPLETE: INFY data (915 points) appended. Total: 115486 ---\n",
            "--- COMPLETE: OLECTRA data (1215 points) appended. Total: 116701 ---\n",
            "--- COMPLETE: HDFCBANK data (763 points) appended. Total: 117464 ---\n",
            "--- COMPLETE: GODREJAGRO data (669 points) appended. Total: 118133 ---\n",
            "--- COMPLETE: INDIAMART data (679 points) appended. Total: 118812 ---\n",
            "--- COMPLETE: LEMONTREE data (760 points) appended. Total: 119572 ---\n",
            "--- COMPLETE: METROBRAND data (393 points) appended. Total: 119965 ---\n",
            "--- COMPLETE: COALINDIA data (827 points) appended. Total: 120792 ---\n",
            "--- COMPLETE: CGCL data (976 points) appended. Total: 121768 ---\n",
            "--- COMPLETE: MAZDOCK data (743 points) appended. Total: 122511 ---\n",
            "--- COMPLETE: KFINTECH data (284 points) appended. Total: 122795 ---\n",
            "--- COMPLETE: CHEMPLASTS data (308 points) appended. Total: 123103 ---\n",
            "--- COMPLETE: ESCORTS data (1382 points) appended. Total: 124485 ---\n",
            "--- COMPLETE: SCI data (1199 points) appended. Total: 125684 ---\n",
            "--- COMPLETE: TECHNOE data (591 points) appended. Total: 126275 ---\n",
            "--- COMPLETE: ICICIBANK data (903 points) appended. Total: 127178 ---\n",
            "--- COMPLETE: SPARC data (1107 points) appended. Total: 128285 ---\n",
            "--- COMPLETE: JKBANK data (1010 points) appended. Total: 129295 ---\n",
            "--- COMPLETE: HONAUT data (826 points) appended. Total: 130121 ---\n",
            "--- COMPLETE: DEEPAKFERT data (1195 points) appended. Total: 131316 ---\n",
            "--- COMPLETE: NCC data (1222 points) appended. Total: 132538 ---\n",
            "--- COMPLETE: TATAELXSI data (1397 points) appended. Total: 133935 ---\n",
            "--- COMPLETE: ASIANPAINT data (969 points) appended. Total: 134904 ---\n",
            "--- COMPLETE: SHRIRAMFIN data (1051 points) appended. Total: 135955 ---\n",
            "--- COMPLETE: NETWORK18 data (1046 points) appended. Total: 137001 ---\n",
            "--- COMPLETE: TVSSCS data (181 points) appended. Total: 137182 ---\n",
            "--- COMPLETE: JSL data (1047 points) appended. Total: 138229 ---\n",
            "--- COMPLETE: BLUESTARCO data (973 points) appended. Total: 139202 ---\n",
            "--- COMPLETE: MGL data (934 points) appended. Total: 140136 ---\n",
            "--- COMPLETE: HONASA data (176 points) appended. Total: 140312 ---\n",
            "--- COMPLETE: DELHIVERY data (329 points) appended. Total: 140641 ---\n",
            "--- COMPLETE: ABCAPITAL data (753 points) appended. Total: 141394 ---\n",
            "--- COMPLETE: HDFCLIFE data (658 points) appended. Total: 142052 ---\n",
            "--- COMPLETE: SOLARINDS data (759 points) appended. Total: 142811 ---\n",
            "--- COMPLETE: AARTIIND data (978 points) appended. Total: 143789 ---\n",
            "--- COMPLETE: GRASIM data (868 points) appended. Total: 144657 ---\n",
            "--- COMPLETE: TIMKEN data (931 points) appended. Total: 145588 ---\n",
            "--- COMPLETE: SJVN data (817 points) appended. Total: 146405 ---\n",
            "--- COMPLETE: JMFINANCIL data (1042 points) appended. Total: 147447 ---\n",
            "--- COMPLETE: IPCALAB data (984 points) appended. Total: 148431 ---\n",
            "--- COMPLETE: SOBHA data (1128 points) appended. Total: 149559 ---\n",
            "--- COMPLETE: NMDC data (1032 points) appended. Total: 150591 ---\n",
            "--- COMPLETE: GAEL data (1003 points) appended. Total: 151594 ---\n",
            "--- COMPLETE: IEX data (734 points) appended. Total: 152328 ---\n",
            "--- COMPLETE: MARUTI data (1056 points) appended. Total: 153384 ---\n",
            "--- COMPLETE: UNIONBANK data (1062 points) appended. Total: 154446 ---\n",
            "--- COMPLETE: AWL data (343 points) appended. Total: 154789 ---\n",
            "--- COMPLETE: SBILIFE data (596 points) appended. Total: 155385 ---\n",
            "--- COMPLETE: QUESS data (896 points) appended. Total: 156281 ---\n",
            "--- COMPLETE: AFFLE data (748 points) appended. Total: 157029 ---\n",
            "--- COMPLETE: SIGNATURE data (151 points) appended. Total: 157180 ---\n",
            "--- COMPLETE: AVANTIFEED data (1069 points) appended. Total: 158249 ---\n",
            "--- COMPLETE: ANANTRAJ data (1046 points) appended. Total: 159295 ---\n",
            "--- COMPLETE: CDSL data (933 points) appended. Total: 160228 ---\n",
            "--- COMPLETE: RHIM data (843 points) appended. Total: 161071 ---\n",
            "--- COMPLETE: NAUKRI data (1045 points) appended. Total: 162116 ---\n",
            "--- COMPLETE: VIPIND data (1202 points) appended. Total: 163318 ---\n",
            "--- COMPLETE: TECHM data (891 points) appended. Total: 164209 ---\n",
            "--- COMPLETE: CAMPUS data (301 points) appended. Total: 164510 ---\n",
            "--- COMPLETE: ZEEL data (1083 points) appended. Total: 165593 ---\n",
            "--- COMPLETE: NIACL data (770 points) appended. Total: 166363 ---\n",
            "--- COMPLETE: GSPL data (996 points) appended. Total: 167359 ---\n",
            "--- COMPLETE: JSWSTEEL data (1264 points) appended. Total: 168623 ---\n",
            "--- COMPLETE: ALOKINDS data (470 points) appended. Total: 169093 ---\n",
            "--- COMPLETE: BANKINDIA data (1109 points) appended. Total: 170202 ---\n",
            "--- COMPLETE: JUBLPHARMA data (1286 points) appended. Total: 171488 ---\n",
            "--- COMPLETE: NAVINFLUOR data (966 points) appended. Total: 172454 ---\n",
            "--- COMPLETE: CARBORUNIV data (832 points) appended. Total: 173286 ---\n",
            "--- COMPLETE: OBEROIRLTY data (1048 points) appended. Total: 174334 ---\n",
            "--- COMPLETE: PVRINOX data (1268 points) appended. Total: 175602 ---\n",
            "--- COMPLETE: TORNTPHARM data (919 points) appended. Total: 176521 ---\n",
            "--- COMPLETE: GPIL data (1196 points) appended. Total: 177717 ---\n",
            "--- COMPLETE: LICI data (303 points) appended. Total: 178020 ---\n",
            "--- COMPLETE: SUNDRMFAST data (893 points) appended. Total: 178913 ---\n",
            "--- COMPLETE: JBMA data (1032 points) appended. Total: 179945 ---\n",
            "--- COMPLETE: TITAGARH data (1195 points) appended. Total: 181140 ---\n",
            "--- COMPLETE: LUPIN data (964 points) appended. Total: 182104 ---\n",
            "--- COMPLETE: SONACOMS data (400 points) appended. Total: 182504 ---\n",
            "--- COMPLETE: OIL data (990 points) appended. Total: 183494 ---\n",
            "--- COMPLETE: PNBHOUSING data (918 points) appended. Total: 184412 ---\n",
            "--- COMPLETE: HAPPSTMNDS data (528 points) appended. Total: 184940 ---\n",
            "--- COMPLETE: BANKBARODA data (1038 points) appended. Total: 185978 ---\n",
            "--- COMPLETE: PIDILITIND data (958 points) appended. Total: 186936 ---\n",
            "--- COMPLETE: VIJAYA data (395 points) appended. Total: 187331 ---\n",
            "--- COMPLETE: ABFRL data (1063 points) appended. Total: 188394 ---\n",
            "--- COMPLETE: SAPPHIRE data (363 points) appended. Total: 188757 ---\n",
            "--- COMPLETE: NBCC data (1080 points) appended. Total: 189837 ---\n",
            "--- COMPLETE: KOTAKBANK data (772 points) appended. Total: 190609 ---\n",
            "--- COMPLETE: PFC data (1017 points) appended. Total: 191626 ---\n",
            "--- COMPLETE: POLYMED data (980 points) appended. Total: 192606 ---\n",
            "--- COMPLETE: WELSPUNLIV data (1122 points) appended. Total: 193728 ---\n",
            "--- COMPLETE: VBL data (805 points) appended. Total: 194533 ---\n",
            "--- COMPLETE: BAJAJHLDNG data (942 points) appended. Total: 195475 ---\n",
            "--- COMPLETE: ABBOTINDIA data (802 points) appended. Total: 196277 ---\n",
            "--- COMPLETE: ASHOKLEY data (1121 points) appended. Total: 197398 ---\n",
            "--- COMPLETE: METROPOLIS data (578 points) appended. Total: 197976 ---\n",
            "--- COMPLETE: HINDCOPPER data (1137 points) appended. Total: 199113 ---\n",
            "--- COMPLETE: MAHSEAMLES data (875 points) appended. Total: 199988 ---\n",
            "--- COMPLETE: BIOCON data (1197 points) appended. Total: 201185 ---\n",
            "--- COMPLETE: AMBER data (826 points) appended. Total: 202011 ---\n",
            "--- COMPLETE: AADHARHFC data (134 points) appended. Total: 202145 ---\n",
            "--- COMPLETE: ASAHIINDIA data (801 points) appended. Total: 202946 ---\n",
            "--- COMPLETE: MANAPPURAM data (1216 points) appended. Total: 204162 ---\n",
            "--- COMPLETE: TATAPOWER data (1111 points) appended. Total: 205273 ---\n",
            "--- COMPLETE: ZFCVINDIA data (751 points) appended. Total: 206024 ---\n",
            "--- COMPLETE: PGHH data (674 points) appended. Total: 206698 ---\n",
            "--- COMPLETE: EMAMILTD data (913 points) appended. Total: 207611 ---\n",
            "--- COMPLETE: ITC data (761 points) appended. Total: 208372 ---\n",
            "--- COMPLETE: UCOBANK data (743 points) appended. Total: 209115 ---\n",
            "--- COMPLETE: HAVELLS data (1060 points) appended. Total: 210175 ---\n",
            "--- COMPLETE: DEEPAKNTR data (1075 points) appended. Total: 211250 ---\n",
            "--- COMPLETE: NYKAA data (300 points) appended. Total: 211550 ---\n",
            "--- COMPLETE: IRCON data (761 points) appended. Total: 212311 ---\n",
            "--- COMPLETE: UNOMINDA data (1020 points) appended. Total: 213331 ---\n",
            "--- COMPLETE: SRF data (1184 points) appended. Total: 214515 ---\n",
            "--- COMPLETE: PPLPHARMA data (285 points) appended. Total: 214800 ---\n",
            "--- COMPLETE: GAIL data (883 points) appended. Total: 215683 ---\n",
            "--- COMPLETE: LINDEINDIA data (1038 points) appended. Total: 216721 ---\n",
            "--- COMPLETE: SUNPHARMA data (897 points) appended. Total: 217618 ---\n",
            "--- COMPLETE: CANBK data (1231 points) appended. Total: 218849 ---\n",
            "--- COMPLETE: NETWEB data (239 points) appended. Total: 219088 ---\n",
            "--- COMPLETE: ATUL data (901 points) appended. Total: 219989 ---\n",
            "--- COMPLETE: SHREECEM data (917 points) appended. Total: 220906 ---\n",
            "--- COMPLETE: YESBANK data (917 points) appended. Total: 221823 ---\n",
            "--- COMPLETE: GUJGASLTD data (921 points) appended. Total: 222744 ---\n",
            "--- COMPLETE: GILLETTE data (762 points) appended. Total: 223506 ---\n",
            "--- COMPLETE: KSB data (828 points) appended. Total: 224334 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0aDeTtDQGgO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section AFTER COMPLETION"
      ],
      "metadata": {
        "id": "VRJGL4UZGhQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# --- Colab Setup and Data Configuration ---\n",
        "# 1. Google Drive Path for permanent storage\n",
        "\n",
        "# 2. **REQUIRED: Update this with a fresh, unexpired link from the Kaggle dataset page**\n",
        "# NOTE: This link is only used if the data is NOT found in Google Drive.\n",
        "DATA_URL = \"https://storage.googleapis.com/kaggle-data-sets/2575525/12691112/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20251001%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251001T235419Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0f9c7fd81c9801a691e9bed7879114c4c122987a0ed10445403f0dae54d50d333adaa1d0bf0af42b95facb7860a6d3d61a866409b2574ba9223c3c5ae97aea974508e75e9414c6ab9f021e4e30b58b642902734bd4641f9592967f5caa4e92d7e521481da9595f4a2b30c0d6b6dd48a8e07d357386ae3594335aa2255f2c7862398e7d11086c46cca0e9698bdcbca9ae6b0a583f076699989cfbaef2e5486873e97a01f029f8545f502354ce75139f01362260f543101ed6f32632452054831e820c7e67846d4800d69092b482c9a8992f1b509c99d4774c8e419c8c97b61b25f0ae591881ee45cd4e2b55779ba8ad98d7fd2a9cf3619926937851939e0dad53\"\n",
        "DATA_DIR=\"/content/NSE_STOCK_DATA\" #/content/NSE_STOCK_DATA\n",
        "# --- Strategy Parameters ---\n",
        "# Primary timeframe for signal generation. Features are looked up relative to this TF.\n",
        "PRIMARY_TIMEFRAME = 3\n",
        "# All timeframes to load, calculate indicators for, and use for MTF confluence lookup\n",
        "ALL_TIMEFRAMES = [3, 5, 15, 30, 60, 1440] # 1440min = 1 Day\n",
        "ATR_PERIOD = 14\n",
        "BB_PERIOD = 20\n",
        "KC_PERIOD = 20\n",
        "KC_MULTIPLIER = 2.0\n",
        "RVOL_THRESHOLD = 2.0 # Strict RVOL filter\n",
        "\n",
        "# --- Helper Functions (Indicators) ---\n",
        "\n",
        "def calculate_vwap(df):\n",
        "    \"\"\"Calculates Volume Weighted Average Price (VWAP) for the trading day.\"\"\"\n",
        "    df['date_only'] = df.index.date\n",
        "    # Calculate typical price (High + Low + Close) / 3\n",
        "    df['tp'] = (df['high'] + df['low'] + df['close']) / 3\n",
        "    # Group by trading day\n",
        "    daily_groups = df.groupby('date_only')\n",
        "\n",
        "    # Calculate VWAP cumulatively for each day\n",
        "    df['tp_vol'] = df['tp'] * df['volume']\n",
        "\n",
        "    # Cumulative Sums for each day\n",
        "    df['cumulative_tp_vol'] = daily_groups['tp_vol'].cumsum()\n",
        "    df['cumulative_volume'] = daily_groups['volume'].cumsum()\n",
        "\n",
        "    # VWAP = Cumulative(TP * Volume) / Cumulative(Volume)\n",
        "    df['vwap'] = df['cumulative_tp_vol'] / df['cumulative_volume']\n",
        "\n",
        "    df.drop(columns=['date_only', 'tp', 'tp_vol', 'cumulative_tp_vol', 'cumulative_volume'], inplace=True)\n",
        "    return df\n",
        "\n",
        "def calculate_rsi(df, period=14):\n",
        "    \"\"\"Calculates Relative Strength Index (RSI).\"\"\"\n",
        "    delta = df['close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).ewm(span=period, min_periods=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).ewm(span=period, min_periods=period).mean()\n",
        "    rs = gain / loss\n",
        "    df['rsi'] = 100 - (100 / (1 + rs))\n",
        "    return df\n",
        "\n",
        "def calculate_adx(df, period=14):\n",
        "    \"\"\"Calculates Average Directional Index (ADX).\"\"\"\n",
        "    # Calculate True Range (TR)\n",
        "    high_minus_low = df['high'] - df['low']\n",
        "    high_minus_prev_close = abs(df['high'] - df['close'].shift(1))\n",
        "    low_minus_prev_close = abs(df['low'] - df['close'].shift(1))\n",
        "    df['tr'] = pd.DataFrame({'a': high_minus_low, 'b': high_minus_prev_close, 'c': low_minus_prev_close}).max(axis=1)\n",
        "\n",
        "    # Calculate ATR (Average True Range)\n",
        "    df['atr'] = df['tr'].ewm(span=period, adjust=False).mean()\n",
        "\n",
        "    # Calculate Directional Movement (+DM and -DM)\n",
        "    high_diff = df['high'].diff()\n",
        "    low_diff = df['low'].diff()\n",
        "\n",
        "    df['dm_plus'] = np.where(high_diff > low_diff, np.maximum(high_diff, 0), 0)\n",
        "    df['dm_minus'] = np.where(low_diff > high_diff, np.maximum(low_diff, 0), 0)\n",
        "\n",
        "    # Calculate Smoothed Directional Movement\n",
        "    df['di_plus'] = (df['dm_plus'].ewm(span=period, adjust=False).mean() / df['atr']) * 100\n",
        "    df['di_minus'] = (df['dm_minus'].ewm(span=period, adjust=False).mean() / df['atr']) * 100\n",
        "\n",
        "    # Calculate Directional Index (DX)\n",
        "    df['dx'] = abs(df['di_plus'] - df['di_minus']) / (df['di_plus'] + df['di_minus']) * 100\n",
        "\n",
        "    # Calculate ADX (Average Directional Index)\n",
        "    df['adx'] = df['dx'].ewm(span=period, adjust=False).mean()\n",
        "\n",
        "    df.drop(columns=['tr', 'dm_plus', 'dm_minus', 'di_plus', 'di_minus', 'dx'], inplace=True, errors='ignore')\n",
        "    return df\n",
        "\n",
        "def calculate_indicators(df, atr_period=ATR_PERIOD, bb_period=BB_PERIOD, kc_period=KC_PERIOD, kc_multiplier=KC_MULTIPLIER):\n",
        "    \"\"\"\n",
        "    Calculates primary indicators including ATR, BB, KC, RVOL, RSI, ADX, and SMAs.\n",
        "    \"\"\"\n",
        "    # 1. True Range and ATR (Needed for KC and Normalization)\n",
        "    df['high_low'] = df['high'] - df['low']\n",
        "    df['high_prev_close'] = abs(df['high'] - df['close'].shift(1))\n",
        "    df['low_prev_close'] = abs(df['low'] - df['close'].shift(1))\n",
        "    df['tr'] = df[['high_low', 'high_prev_close', 'low_prev_close']].max(axis=1)\n",
        "    df['atr'] = df['tr'].ewm(alpha=1/atr_period, adjust=False).mean()\n",
        "\n",
        "    # 2. Bollinger Bands (BB)\n",
        "    df['bb_sma'] = df['close'].rolling(window=bb_period).mean()\n",
        "    df['bb_std'] = df['close'].rolling(window=bb_period).std()\n",
        "    df['bb_upper'] = df['bb_sma'] + (df['bb_std'] * 2)\n",
        "    df['bb_lower'] = df['bb_sma'] - (df['bb_std'] * 2)\n",
        "\n",
        "    # 3. Keltner Channels (KC)\n",
        "    df['kc_sma'] = df['close'].rolling(window=kc_period).mean()\n",
        "    df['kc_upper'] = df['kc_sma'] + (df['atr'] * kc_multiplier)\n",
        "    df['kc_lower'] = df['kc_sma'] - (df['atr'] * kc_multiplier)\n",
        "\n",
        "    # 4. Squeeze Status\n",
        "    df['squeeze_on'] = (df['bb_lower'] > df['kc_lower']) & (df['bb_upper'] < df['kc_upper'])\n",
        "\n",
        "    # 5. Relative Volume (RVOL)\n",
        "    df['avg_volume'] = df['volume'].rolling(window=20).mean()\n",
        "    df['rvol'] = df['volume'] / df['avg_volume']\n",
        "\n",
        "    # 6. RSI and ADX\n",
        "    df = calculate_rsi(df, period=14)\n",
        "    df = calculate_adx(df, period=14)\n",
        "\n",
        "    # 7. SMAs and Normalized Distance (Requires 'atr' to be calculated first)\n",
        "    df['sma_50'] = df['close'].rolling(window=50).mean()\n",
        "    df['sma_200'] = df['close'].rolling(window=200).mean()\n",
        "    # Normalized distance from MA to close, normalized by ATR\n",
        "    df['dist_from_sma_50'] = (df['close'] - df['sma_50']) / df['atr']\n",
        "    df['dist_from_sma_200'] = (df['close'] - df['sma_200']) / df['atr']\n",
        "\n",
        "    # 8. VWAP\n",
        "    df = calculate_vwap(df)\n",
        "\n",
        "    # Drop intermediate columns\n",
        "    cols_to_drop = ['high_low', 'high_prev_close', 'low_prev_close', 'tr', 'avg_volume', 'bb_sma', 'bb_std', 'kc_sma', 'sma_50', 'sma_200']\n",
        "    df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True, errors='ignore')\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- Data Loading and Resampling ---\n",
        "\n",
        "def load_and_resample_data(filepath, timeframe_minutes):\n",
        "    \"\"\"\n",
        "    Loads 1-minute data and resamples it to the specified timeframe.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load 1-minute data\n",
        "        df = pd.read_csv(filepath, index_col='date', parse_dates=True)\n",
        "\n",
        "        # Resample to the target timeframe\n",
        "        resample_period = f'{timeframe_minutes}min'\n",
        "        resampled_df = df.resample(resample_period).agg({\n",
        "            'open': 'first',\n",
        "            'high': 'max',\n",
        "            'low': 'min',\n",
        "            'close': 'last',\n",
        "            'volume': 'sum'\n",
        "        }).dropna()\n",
        "\n",
        "        return resampled_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading/resampling {filepath} to {timeframe_minutes}min: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Training Data Generation (Core Logic) ---\n",
        "\n",
        "def generate_training_data(df, symbol, timeframe, mtf_data):\n",
        "    \"\"\"\n",
        "    Generates a DataFrame of training data points incorporating MTF and VWAP filters.\n",
        "    \"\"\"\n",
        "    training_data = []\n",
        "\n",
        "    # Get the higher timeframe DataFrames for MTF lookup\n",
        "    df_5m = mtf_data.get(5)\n",
        "    df_15m = mtf_data.get(15)\n",
        "    df_30m = mtf_data.get(30)\n",
        "    df_60m = mtf_data.get(60)\n",
        "    df_1D = mtf_data.get(1440)\n",
        "\n",
        "    # Note: df is the PRIMARY_TIMEFRAME (e.g., 3m)\n",
        "\n",
        "    for i in range(1, len(df)):\n",
        "\n",
        "        # --- Squeeze and Breakout Detection ---\n",
        "        signal = False\n",
        "        direction = None\n",
        "\n",
        "        was_in_squeeze = df['squeeze_on'].iloc[i-1]\n",
        "        is_in_squeeze = df['squeeze_on'].iloc[i]\n",
        "\n",
        "        current_date = df.index[i]\n",
        "\n",
        "        # Bullish Breakout\n",
        "        if was_in_squeeze and not is_in_squeeze and df['close'].iloc[i] > df['bb_upper'].iloc[i]:\n",
        "            direction = 'long'\n",
        "        # Bearish Breakout\n",
        "        elif was_in_squeeze and not is_in_squeeze and df['close'].iloc[i] < df['bb_lower'].iloc[i]:\n",
        "            direction = 'short'\n",
        "\n",
        "        # --- Combined Entry Filters (RVOL & VWAP) ---\n",
        "        if direction:\n",
        "            # 1. RVOL > 2.0 Filter\n",
        "            rvol_ok = df['rvol'].iloc[i] >= RVOL_THRESHOLD\n",
        "\n",
        "            # 2. VWAP Filter\n",
        "            if direction == 'long':\n",
        "                vwap_ok = df['close'].iloc[i] > df['vwap'].iloc[i]\n",
        "            else: # short\n",
        "                vwap_ok = df['close'].iloc[i] < df['vwap'].iloc[i]\n",
        "\n",
        "            signal = rvol_ok and vwap_ok\n",
        "\n",
        "        # --- Final Signal and Trade Simulation ---\n",
        "        if signal and i + 1 < len(df):\n",
        "\n",
        "            # --- MTF Feature Lookup ---\n",
        "\n",
        "            def get_mtf_squeeze(high_df, current_time):\n",
        "                if high_df is not None and not high_df.empty:\n",
        "                    # Use searchsorted for efficient time-based lookup\n",
        "                    idx = high_df.index.searchsorted(current_time, side='right') - 1\n",
        "                    if idx >= 0 and idx < len(high_df):\n",
        "                        return high_df['squeeze_on'].iloc[idx].astype(int)\n",
        "                return 0 # Default to 0 (False) if data is missing or lookup fails\n",
        "\n",
        "            mtf_sqz_5m = get_mtf_squeeze(df_5m, current_date)\n",
        "            mtf_sqz_15m = get_mtf_squeeze(df_15m, current_date)\n",
        "            mtf_sqz_30m = get_mtf_squeeze(df_30m, current_date)\n",
        "            mtf_sqz_60m = get_mtf_squeeze(df_60m, current_date)\n",
        "            mtf_sqz_1D = get_mtf_squeeze(df_1D, current_date)\n",
        "\n",
        "            # Capture features at the time of the signal\n",
        "            features = {\n",
        "                'symbol': symbol,\n",
        "                'timeframe': timeframe,\n",
        "                'date': current_date,\n",
        "                'direction': direction,\n",
        "                'rvol': df['rvol'].iloc[i],\n",
        "                'bb_width': df['bb_upper'].iloc[i] - df['bb_lower'].iloc[i],\n",
        "                'kc_width': df['kc_upper'].iloc[i] - df['kc_lower'].iloc[i],\n",
        "                # New Technical Features\n",
        "                'rsi': df['rsi'].iloc[i],\n",
        "                'adx': df['adx'].iloc[i],\n",
        "                'dist_from_sma_50': df['dist_from_sma_50'].iloc[i],\n",
        "                'dist_from_sma_200': df['dist_from_sma_200'].iloc[i],\n",
        "                # New MTF Confluence Features\n",
        "                'is_5min_sqz': mtf_sqz_5m,\n",
        "                'is_15min_sqz': mtf_sqz_15m,\n",
        "                'is_30min_sqz': mtf_sqz_30m,\n",
        "                'is_60min_sqz': mtf_sqz_60m,\n",
        "                'is_1D_sqz': mtf_sqz_1D,\n",
        "                # VWAP Distance (Normalized)\n",
        "                'dist_from_vwap': (df['close'].iloc[i] - df['vwap'].iloc[i]) / df['atr'].iloc[i]\n",
        "            }\n",
        "\n",
        "            # Simulate the trade (1:2 Risk/Reward) to get the outcome (label)\n",
        "            entry_price = df['open'].iloc[i+1]\n",
        "            label = 0  # Default to loss\n",
        "\n",
        "            # Calculate risk, SL, TP based on breakout candle's ATR\n",
        "            risk = df['atr'].iloc[i] # Use ATR of the signal bar as risk unit\n",
        "\n",
        "            if direction == 'long':\n",
        "                stop_loss = entry_price - risk\n",
        "                take_profit = entry_price + (2 * risk)\n",
        "            else: # short\n",
        "                stop_loss = entry_price + risk\n",
        "                take_profit = entry_price - (2 * risk)\n",
        "\n",
        "            # Look ahead to find the exit (using next bar's high/low)\n",
        "            for j in range(i + 1, len(df)):\n",
        "                current_low = df['low'].iloc[j]\n",
        "                current_high = df['high'].iloc[j]\n",
        "                exit_found = False\n",
        "\n",
        "                if direction == 'long':\n",
        "                    if current_high >= take_profit:\n",
        "                        label = 1 # Success (Target hit)\n",
        "                        exit_found = True\n",
        "                    elif current_low <= stop_loss:\n",
        "                        label = 0 # Failure (Stop Loss hit)\n",
        "                        exit_found = True\n",
        "                else: # short\n",
        "                    if current_low <= take_profit:\n",
        "                        label = 1 # Success (Target hit)\n",
        "                        exit_found = True\n",
        "                    elif current_high >= stop_loss:\n",
        "                        label = 0 # Failure (Stop Loss hit)\n",
        "                        exit_found = True\n",
        "\n",
        "                if exit_found:\n",
        "                    features['outcome'] = label\n",
        "                    training_data.append(features)\n",
        "                    break\n",
        "\n",
        "    return pd.DataFrame(training_data)\n",
        "\n",
        "# --- Parallel Processing Worker ---\n",
        "\n",
        "def process_single_symbol(symbol, data_directory):\n",
        "    \"\"\"\n",
        "    Worker function to process a single symbol across all timeframes and generate data.\n",
        "    Returns a list of DataFrames (one per timeframe).\n",
        "    \"\"\"\n",
        "    # print(f\"Starting processing for {symbol}...\") # Suppressing print for cleaner output\n",
        "\n",
        "    filename = f\"{symbol}_minute.csv\"\n",
        "    filepath = os.path.join(data_directory, filename)\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        # print(f\"Skipping {symbol}: Data file not found.\")\n",
        "        return []\n",
        "\n",
        "    # 1. Load and Prepare ALL Timeframes for MTF Lookups\n",
        "    mtf_data = {}\n",
        "    for tf in ALL_TIMEFRAMES:\n",
        "        df = load_and_resample_data(filepath, tf)\n",
        "        if df is not None and not df.empty:\n",
        "            df = calculate_indicators(df)\n",
        "            df.dropna(inplace=True)\n",
        "            mtf_data[tf] = df\n",
        "        else:\n",
        "             pass # Suppressing print: Warning: Missing or empty data for {symbol} on {tf}min.\n",
        "\n",
        "    # 2. Generate Training Data (Only use the primary timeframe for signaling)\n",
        "    if PRIMARY_TIMEFRAME not in mtf_data:\n",
        "        # print(f\"Skipping {symbol}: Primary TF ({PRIMARY_TIMEFRAME}min) data is missing.\")\n",
        "        return []\n",
        "\n",
        "    df_primary = mtf_data[PRIMARY_TIMEFRAME]\n",
        "\n",
        "    # This function uses df_primary for signals, and mtf_data for lookups\n",
        "    symbol_dataset = generate_training_data(df_primary, symbol, PRIMARY_TIMEFRAME, mtf_data)\n",
        "\n",
        "    if not symbol_dataset.empty:\n",
        "        # print(f\"Completed {symbol}: Generated {len(symbol_dataset)} data points.\")\n",
        "        return [symbol_dataset]\n",
        "\n",
        "    # print(f\"Completed {symbol}: Generated 0 data points.\")\n",
        "    return []\n",
        "\n",
        "# --- Colab and Setup Functions ---\n",
        "\n",
        "def load_symbols_from_dir(directory):\n",
        "    \"\"\"\n",
        "    Extracts stock symbols from filenames in a directory.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        return []\n",
        "\n",
        "    csv_filenames = [f for f in os.listdir(directory) if f.endswith(\".csv\")]\n",
        "    pattern = r\"(.*)_minute.csv\"\n",
        "    symbols = []\n",
        "    for filename in csv_filenames:\n",
        "        match = re.search(pattern, filename)\n",
        "        if match:\n",
        "            symbols.append(match.group(1))\n",
        "    return symbols\n",
        "\n",
        "def setup_colab_environment(data_url, data_dir):\n",
        "    \"\"\"Downloads and unzips data, checking Google Drive first.\"\"\"\n",
        "\n",
        "    # 1. Mount Google Drive\n",
        "    #print(\"Attempting to mount Google Drive...\")\n",
        "    #from google.colab import drive\n",
        "    #try:\n",
        "    #    drive.mount('/content/drive', force_remount=True)\n",
        "    #    print(\"Google Drive mounted successfully.\")\n",
        "    #except Exception as e:\n",
        "    #    print(f\"Error mounting Google Drive: {e}\")\n",
        "\n",
        "    # 2. Check if data exists in Drive (Persistent Storage)\n",
        "    #if os.path.exists(data_dir) and len(load_symbols_from_dir(data_dir)) > 50:\n",
        "    #    print(f\"\\n Data already found in Google Drive: {data_dir}. Skipping download.\")\n",
        "    #    return True # Data is ready\n",
        "\n",
        "    # 3. Data Not Found - Proceed to Download\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    zip_path = os.path.join(data_dir, 'data_archive.zip')\n",
        "\n",
        "    # Use 'curl -L' for robust downloading of signed URLs\n",
        "    print(f\"\\n Data not found. Downloading ZIP from URL...\")\n",
        "    os.system(f\"curl -L '{data_url}' -o '{zip_path}'\")\n",
        "\n",
        "    if not os.path.exists(zip_path) or os.path.getsize(zip_path) < 1024 * 1024:\n",
        "        print(\"\\nFATAL ERROR: Download failed or file is too small (likely an expired URL).\")\n",
        "        print(\"Please obtain a new, fresh download link from the Kaggle dataset page.\")\n",
        "        return False\n",
        "\n",
        "    if not zipfile.is_zipfile(zip_path):\n",
        "        print(\"\\nFATAL ERROR: Downloaded file is not a zip file (likely an HTML error page from an expired URL).\")\n",
        "        print(\"Please obtain a new, fresh download link from the Kaggle dataset page.\")\n",
        "        os.remove(zip_path)\n",
        "        return False\n",
        "\n",
        "    print(f\"Unzipping data into {data_dir}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "\n",
        "    os.remove(zip_path)\n",
        "    print(\"Download and extraction complete.\")\n",
        "    return True\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "\n",
        "def generate_and_train_pipeline(data_url, data_dir, output_dir):\n",
        "    \"\"\"\n",
        "    Full pipeline: Setup, Generation, Training, and Evaluation.\n",
        "    \"\"\"\n",
        "    output_path = os.path.join(output_dir, \"training_dataset.csv\")\n",
        "    model_output_path = os.path.join(output_dir, \"squeeze_classifier_model.pkl\")\n",
        "\n",
        "\n",
        "    # 1. --- SETUP ENVIRONMENT & DATA DOWNLOAD/GENERATION (COMMENTED OUT FOR FASTER RETRAINING) ---\n",
        "\n",
        "    if not setup_colab_environment(data_url, data_dir):\n",
        "        return\n",
        "\n",
        "    all_symbols = load_symbols_from_dir(data_dir)\n",
        "    if not all_symbols:\n",
        "        print(f\"FATAL ERROR: No stock data files found in {data_dir}.\")\n",
        "        return\n",
        "        print(f\"\\n--- Starting Parallel Data Generation for {len(all_symbols)} symbols ---\")\n",
        "        # Write header once before parallel processing starts\n",
        "    header_cols = [\n",
        "        'symbol', 'timeframe', 'date', 'direction', 'rvol', 'bb_width', 'kc_width',\n",
        "        'rsi', 'adx', 'dist_from_sma_50', 'dist_from_sma_200', 'is_5min_sqz',\n",
        "        'is_15min_sqz', 'is_30min_sqz', 'is_60min_sqz', 'is_1D_sqz',\n",
        "        'dist_from_vwap', 'outcome'\n",
        "    ]\n",
        "    pd.DataFrame(columns=header_cols).to_csv(output_path, index=False)\n",
        "    print(f\"Created new training dataset file: {output_path}\")\n",
        "\n",
        "    total_data_points = 0\n",
        "    start_time = datetime.now()\n",
        "        # Use ProcessPoolExecutor to leverage all available CPU cores\n",
        "    with ProcessPoolExecutor(max_workers=None) as executor:\n",
        "        future_to_symbol = {executor.submit(process_single_symbol, symbol, data_dir): symbol for symbol in all_symbols}\n",
        "\n",
        "        for future in as_completed(future_to_symbol):\n",
        "            symbol = future_to_symbol[future]\n",
        "            try:\n",
        "                results = future.result()\n",
        "                for df in results:\n",
        "                    # Append results sequentially to prevent file corruption\n",
        "                    df.to_csv(output_path, mode='a', header=False, index=False)\n",
        "                    total_data_points += len(df)\n",
        "            except Exception as e:\n",
        "                print(f\"!!! Error processing symbol {symbol}: {e}\")\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print(f\"\\n--- Data generation complete in {end_time - start_time} ---\")\n",
        "    print(f\"Total data points saved to {output_path}: {total_data_points}\")\n",
        "\n",
        "    # 2. --- MODEL TRAINING and EVALUATION ---\n",
        "\n",
        "    print(\"\\n--- Starting Model Training and Evaluation ---\")\n",
        "\n",
        "    # # We must remount the drive to ensure the training_dataset.csv is accessible\n",
        "    # print(\"Attempting to remount Google Drive to access training data...\")\n",
        "    # try:\n",
        "    #     from google.colab import drive\n",
        "    #     drive.mount('/content/drive', force_remount=True)\n",
        "    #     print(\"Drive remounted successfully.\")\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error remounting Google Drive: {e}. Cannot load data.\")\n",
        "    #     return\n",
        "\n",
        "    # Load the existing generated data\n",
        "    try:\n",
        "        data = pd.read_csv(output_path)\n",
        "    except Exception as e:\n",
        "        #print(f\"FATAL ERROR: Could not load training dataset from Drive at {output_path}. Did you run the generation step previously?\")\n",
        "        print(f\"Error: {e}\")\n",
        "        return\n",
        "\n",
        "    # Drop rows with NaN (which come from MTF lookups where higher TF data is missing)\n",
        "    data.dropna(inplace=True)\n",
        "\n",
        "    # Define features (X) and label (y)\n",
        "    categorical_features = ['direction']\n",
        "    numeric_features = [\n",
        "        'rvol', 'bb_width', 'kc_width', 'rsi', 'adx',\n",
        "        'dist_from_sma_50', 'dist_from_sma_200', 'dist_from_vwap',\n",
        "        'is_5min_sqz', 'is_15min_sqz', 'is_30min_sqz', 'is_60min_sqz', 'is_1D_sqz'\n",
        "    ]\n",
        "\n",
        "    X_features = categorical_features + numeric_features\n",
        "    X = data[X_features]\n",
        "    y = data['outcome']\n",
        "\n",
        "    # Ensure all MTF squeeze columns are treated as integer (0 or 1)\n",
        "    for col in [c for c in X.columns if c.startswith('is_')]:\n",
        "        X[col] = X[col].astype(int)\n",
        "\n",
        "    # Preprocessing Pipeline\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numeric_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    # Split data using time-based split (Train on earlier data, Test on later data)\n",
        "    data.sort_values(by='date', inplace=True)\n",
        "    train_size = int(0.8 * len(data))\n",
        "    X_train = X.iloc[:train_size]\n",
        "    X_test = X.iloc[train_size:]\n",
        "    y_train = y.iloc[:train_size]\n",
        "    y_test = y.iloc[train_size:]\n",
        "\n",
        "    print(f\"Loaded {len(data)} total data points.\")\n",
        "    print(f\"Training set size: {len(X_train)} (80%)\")\n",
        "    print(f\"Testing set size: {len(X_test)} (20%)\")\n",
        "\n",
        "    # Calculate the scale_pos_weight for XGBoost (Ratio of negative to positive examples)\n",
        "    n_neg = (y_train == 0).sum()\n",
        "    n_pos = (y_train == 1).sum()\n",
        "\n",
        "    # Aggressively weight the positive class (Win) to boost Recall (CRITICAL for imbalanced data)\n",
        "    # We are using a multiplier of 2.0 to aggressively force the model to predict 'Win' more often.\n",
        "    scale_pos_weight = (n_neg / n_pos) * 2.0\n",
        "\n",
        "    print(f\"Positive samples (Win): {n_pos}, Negative samples (Loss): {n_neg}\")\n",
        "    print(f\"Calculated scale_pos_weight (Aggressive): {scale_pos_weight:.2f}\")\n",
        "\n",
        "    # Define the Model (XGBoost Classifier) - Parameters tuned for high Recall\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        n_estimators=300,           # Increased estimators for better fit\n",
        "        max_depth=4,                # Slightly reduced depth for better generalization\n",
        "        learning_rate=0.05,         # Slightly reduced learning rate\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        # --- Imbalance/Regularization Tuning for high Recall ---\n",
        "        scale_pos_weight=scale_pos_weight, # AGGRESSIVE WEIGHTING\n",
        "        min_child_weight=5,                # Regularization to prevent overfitting to noise\n",
        "        gamma=0.2,                         # Added gamma for more controlled regularization\n",
        "        tree_method='hist',         # Faster training method\n",
        "        random_state=42,\n",
        "        n_jobs=-1                   # Use all CPU cores\n",
        "    )\n",
        "\n",
        "    # Full Pipeline: Preprocessor -> Model\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('classifier', xgb_model)])\n",
        "\n",
        "    print(\"\\n--- Training XGBoost Classifier (Aggressively Tuned for Recall) ---\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # 3. Evaluation\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(\"\\n--- Model Evaluation on Test Data ---\")\n",
        "\n",
        "    report = classification_report(y_test, y_pred, target_names=['Loss (0)', 'Win (1)'], digits=4)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"\\nConfusion Matrix (Rows=Actual, Cols=Predicted):\")\n",
        "    print(\"[[True Negatives, False Positives]\")\n",
        "    print(\" [False Negatives, True Positives]]\")\n",
        "    print(cm)\n",
        "\n",
        "    accuracy = (cm[0, 0] + cm[1, 1]) / np.sum(cm)\n",
        "    win_precision = cm[1, 1] / (cm[1, 1] + cm[0, 1]) if (cm[1, 1] + cm[0, 1]) > 0 else 0\n",
        "    auc_score = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f} (Overall correct predictions)\")\n",
        "    print(f\"AUC Score: {auc_score:.4f} (Measure of separability between classes)\")\n",
        "    print(f\"\\nPrecision (Win Class): {win_precision:.4f}\")\n",
        "    print(\"--> This is your model's estimated **Win Rate** among the trades it chooses to take.\")\n",
        "\n",
        "    # 4. Save Model\n",
        "    joblib.dump(pipeline, model_output_path)\n",
        "    print(f\"\\nModel saved successfully as '{model_output_path}'\")\n",
        "\n",
        "# Execute the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    generate_and_train_pipeline(DATA_URL, DATA_DIR, DATA_DIR)\n"
      ],
      "metadata": {
        "id": "Hh7EfKEKFpP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fddad0a-0c99-4722-e186-9dc3f7dfff51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Data generation complete in 1:16:56.086540 ---\n",
            "Total data points saved to /content/NSE_STOCK_DATA/training_dataset.csv: 728268\n",
            "\n",
            "--- Starting Model Training and Evaluation ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3087811327.py:500: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = X[col].astype(int)\n",
            "/tmp/ipython-input-3087811327.py:500: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = X[col].astype(int)\n",
            "/tmp/ipython-input-3087811327.py:500: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = X[col].astype(int)\n",
            "/tmp/ipython-input-3087811327.py:500: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = X[col].astype(int)\n",
            "/tmp/ipython-input-3087811327.py:500: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = X[col].astype(int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 728268 total data points.\n",
            "Training set size: 582614 (80%)\n",
            "Testing set size: 145654 (20%)\n",
            "Positive samples (Win): 182073, Negative samples (Loss): 400541\n",
            "Calculated scale_pos_weight (Aggressive): 4.40\n",
            "\n",
            "--- Training XGBoost Classifier (Aggressively Tuned for Recall) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [13:10:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete.\n",
            "\n",
            "--- Model Evaluation on Test Data ---\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Loss (0)     0.8679    0.0090    0.0178    100600\n",
            "     Win (1)     0.3106    0.9969    0.4736     45054\n",
            "\n",
            "    accuracy                         0.3146    145654\n",
            "   macro avg     0.5893    0.5030    0.2457    145654\n",
            "weighted avg     0.6955    0.3146    0.1588    145654\n",
            "\n",
            "\n",
            "Confusion Matrix (Rows=Actual, Cols=Predicted):\n",
            "[[True Negatives, False Positives]\n",
            " [False Negatives, True Positives]]\n",
            "[[  907 99693]\n",
            " [  138 44916]]\n",
            "\n",
            "Accuracy: 0.3146 (Overall correct predictions)\n",
            "AUC Score: 0.5726 (Measure of separability between classes)\n",
            "\n",
            "Precision (Win Class): 0.3106\n",
            "--> This is your model's estimated **Win Rate** among the trades it chooses to take.\n",
            "\n",
            "Model saved successfully as '/content/NSE_STOCK_DATA/squeeze_classifier_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DONEEEEEEEEEEE\")"
      ],
      "metadata": {
        "id": "QnZTvk1f3R2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117ffc53-2999-4f09-d65d-400700512826"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONEEEEEEEEEEE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pkuDkkW_3ReQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}